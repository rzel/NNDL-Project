{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "training @ iter =  0\n",
      "2.30258512497\n",
      "training @ iter =  100\n",
      "2.27059316635\n",
      "training @ iter =  200\n",
      "2.28191184998\n",
      "training @ iter =  300\n",
      "2.22936940193\n",
      "training @ iter =  400\n",
      "2.1809823513\n",
      "epoch 1, minibatch 500/500, test error 81.040000 %\n",
      "training @ iter =  500\n",
      "2.23568964005\n",
      "training @ iter =  600\n",
      "2.03760766983\n",
      "training @ iter =  700\n",
      "2.01883673668\n",
      "training @ iter =  800\n",
      "2.02564525604\n",
      "training @ iter =  900\n",
      "1.77647519112\n",
      "epoch 2, minibatch 500/500, test error 64.640000 %\n",
      "training @ iter =  1000\n",
      "1.91407442093\n",
      "training @ iter =  1100\n",
      "1.80406498909\n",
      "training @ iter =  1200\n",
      "1.84993779659\n",
      "training @ iter =  1300\n",
      "1.72634029388\n",
      "training @ iter =  1400\n",
      "1.54070079327\n",
      "epoch 3, minibatch 500/500, test error 56.180000 %\n",
      "training @ iter =  1500\n",
      "1.70304536819\n",
      "training @ iter =  1600\n",
      "1.68372035027\n",
      "training @ iter =  1700\n",
      "1.68699431419\n",
      "training @ iter =  1800\n",
      "1.59736049175\n",
      "training @ iter =  1900\n",
      "1.27530324459\n",
      "epoch 4, minibatch 500/500, test error 49.610000 %\n",
      "training @ iter =  2000\n",
      "1.44106137753\n",
      "training @ iter =  2100\n",
      "1.55951654911\n",
      "training @ iter =  2200\n",
      "1.66147828102\n",
      "training @ iter =  2300\n",
      "1.37023198605\n",
      "training @ iter =  2400\n",
      "1.2285888195\n",
      "epoch 5, minibatch 500/500, test error 46.000000 %\n",
      "training @ iter =  2500\n",
      "1.38106429577\n",
      "training @ iter =  2600\n",
      "1.27276790142\n",
      "training @ iter =  2700\n",
      "1.39170312881\n",
      "training @ iter =  2800\n",
      "1.33938717842\n",
      "training @ iter =  2900\n",
      "1.05834019184\n",
      "epoch 6, minibatch 500/500, test error 41.450000 %\n",
      "training @ iter =  3000\n",
      "1.26362812519\n",
      "training @ iter =  3100\n",
      "1.13015413284\n",
      "training @ iter =  3200\n",
      "1.25616121292\n",
      "training @ iter =  3300\n",
      "1.30457723141\n",
      "training @ iter =  3400\n",
      "1.05791127682\n",
      "epoch 7, minibatch 500/500, test error 36.720000 %\n",
      "training @ iter =  3500\n",
      "1.14319705963\n",
      "training @ iter =  3600\n",
      "1.17227733135\n",
      "training @ iter =  3700\n",
      "1.35815083981\n",
      "training @ iter =  3800\n",
      "1.17060780525\n",
      "training @ iter =  3900\n",
      "1.05129957199\n",
      "epoch 8, minibatch 500/500, test error 38.310000 %\n",
      "training @ iter =  4000\n",
      "1.15799021721\n",
      "training @ iter =  4100\n",
      "1.10467410088\n",
      "training @ iter =  4200\n",
      "1.13182115555\n",
      "training @ iter =  4300\n",
      "1.04229938984\n",
      "training @ iter =  4400\n",
      "0.8544729352\n",
      "epoch 9, minibatch 500/500, test error 33.420000 %\n",
      "training @ iter =  4500\n",
      "1.07197248936\n",
      "training @ iter =  4600\n",
      "0.991777122021\n",
      "training @ iter =  4700\n",
      "1.10513412952\n",
      "training @ iter =  4800\n",
      "0.976244688034\n",
      "training @ iter =  4900\n",
      "0.863404512405\n",
      "epoch 10, minibatch 500/500, test error 30.510000 %\n",
      "training @ iter =  5000\n",
      "1.01403534412\n",
      "training @ iter =  5100\n",
      "0.971222221851\n",
      "training @ iter =  5200\n",
      "1.09332227707\n",
      "training @ iter =  5300\n",
      "0.980318009853\n",
      "training @ iter =  5400\n",
      "0.864955604076\n",
      "epoch 11, minibatch 500/500, test error 29.260000 %\n",
      "training @ iter =  5500\n",
      "0.876930296421\n",
      "training @ iter =  5600\n",
      "0.793320477009\n",
      "training @ iter =  5700\n",
      "0.910437941551\n",
      "training @ iter =  5800\n",
      "0.820554375648\n",
      "training @ iter =  5900\n",
      "0.684686303139\n",
      "epoch 12, minibatch 500/500, test error 29.720000 %\n",
      "training @ iter =  6000\n",
      "0.997636020184\n",
      "training @ iter =  6100\n",
      "0.767556011677\n",
      "training @ iter =  6200\n",
      "0.946373164654\n",
      "training @ iter =  6300\n",
      "0.797154664993\n",
      "training @ iter =  6400\n",
      "0.748839855194\n",
      "epoch 13, minibatch 500/500, test error 28.210000 %\n",
      "training @ iter =  6500\n",
      "0.89682585001\n",
      "training @ iter =  6600\n",
      "0.786523461342\n",
      "training @ iter =  6700\n",
      "0.83417403698\n",
      "training @ iter =  6800\n",
      "0.877010881901\n",
      "training @ iter =  6900\n",
      "0.690137684345\n",
      "epoch 14, minibatch 500/500, test error 25.220000 %\n",
      "training @ iter =  7000\n",
      "0.81150841713\n",
      "training @ iter =  7100\n",
      "0.590887784958\n",
      "training @ iter =  7200\n",
      "0.801488339901\n",
      "training @ iter =  7300\n",
      "0.885181725025\n",
      "training @ iter =  7400\n",
      "0.734664976597\n",
      "epoch 15, minibatch 500/500, test error 25.410000 %\n",
      "training @ iter =  7500\n",
      "0.90322893858\n",
      "training @ iter =  7600\n",
      "0.766239643097\n",
      "training @ iter =  7700\n",
      "0.855372309685\n",
      "training @ iter =  7800\n",
      "0.806463778019\n",
      "training @ iter =  7900\n",
      "0.818333745003\n",
      "epoch 16, minibatch 500/500, test error 25.020000 %\n",
      "training @ iter =  8000\n",
      "0.86365377903\n",
      "training @ iter =  8100\n",
      "0.785987079144\n",
      "training @ iter =  8200\n",
      "0.871271967888\n",
      "training @ iter =  8300\n",
      "0.773398995399\n",
      "training @ iter =  8400\n",
      "0.817554116249\n",
      "epoch 17, minibatch 500/500, test error 25.800000 %\n",
      "training @ iter =  8500\n",
      "0.842555224895\n",
      "training @ iter =  8600\n",
      "0.821953356266\n",
      "training @ iter =  8700\n",
      "0.831688940525\n",
      "training @ iter =  8800\n",
      "0.923075854778\n",
      "training @ iter =  8900\n",
      "0.697049379349\n",
      "epoch 18, minibatch 500/500, test error 25.090000 %\n",
      "training @ iter =  9000\n",
      "1.01660263538\n",
      "training @ iter =  9100\n",
      "0.755628824234\n",
      "training @ iter =  9200\n",
      "0.70889467001\n",
      "training @ iter =  9300\n",
      "0.713096439838\n",
      "training @ iter =  9400\n",
      "0.772140979767\n",
      "epoch 19, minibatch 500/500, test error 23.930000 %\n",
      "training @ iter =  9500\n",
      "0.793040215969\n",
      "training @ iter =  9600\n",
      "0.717456579208\n",
      "training @ iter =  9700\n",
      "0.778373420238\n",
      "training @ iter =  9800\n",
      "0.79421544075\n",
      "training @ iter =  9900\n",
      "0.675326704979\n",
      "epoch 20, minibatch 500/500, test error 24.930000 %\n",
      "training @ iter =  10000\n",
      "0.921275794506\n",
      "training @ iter =  10100\n",
      "0.767474830151\n",
      "training @ iter =  10200\n",
      "0.760370492935\n",
      "training @ iter =  10300\n",
      "0.690547227859\n",
      "training @ iter =  10400\n",
      "0.55448281765\n",
      "epoch 21, minibatch 500/500, test error 23.460000 %\n",
      "training @ iter =  10500\n",
      "0.861614227295\n",
      "training @ iter =  10600\n",
      "0.656292557716\n",
      "training @ iter =  10700\n",
      "0.759053170681\n",
      "training @ iter =  10800\n",
      "0.836205244064\n",
      "training @ iter =  10900\n",
      "0.68403762579\n",
      "epoch 22, minibatch 500/500, test error 23.330000 %\n",
      "training @ iter =  11000\n",
      "0.764596223831\n",
      "training @ iter =  11100\n",
      "0.663376748562\n",
      "training @ iter =  11200\n",
      "0.7285810709\n",
      "training @ iter =  11300\n",
      "0.913878858089\n",
      "training @ iter =  11400\n",
      "0.686412632465\n",
      "epoch 23, minibatch 500/500, test error 22.960000 %\n",
      "training @ iter =  11500\n",
      "0.735337913036\n",
      "training @ iter =  11600\n",
      "0.830455720425\n",
      "training @ iter =  11700\n",
      "0.726037144661\n",
      "training @ iter =  11800\n",
      "0.593453586102\n",
      "training @ iter =  11900\n",
      "0.607197344303\n",
      "epoch 24, minibatch 500/500, test error 24.310000 %\n",
      "training @ iter =  12000\n",
      "0.748585224152\n",
      "training @ iter =  12100\n",
      "0.748050630093\n",
      "training @ iter =  12200\n",
      "0.705865085125\n",
      "training @ iter =  12300\n",
      "0.639502882957\n",
      "training @ iter =  12400\n",
      "0.531385183334\n",
      "epoch 25, minibatch 500/500, test error 21.820000 %\n",
      "training @ iter =  12500\n",
      "0.899100065231\n",
      "training @ iter =  12600\n",
      "0.676665127277\n",
      "training @ iter =  12700\n",
      "0.637736439705\n",
      "training @ iter =  12800\n",
      "0.882280111313\n",
      "training @ iter =  12900\n",
      "0.555372297764\n",
      "epoch 26, minibatch 500/500, test error 21.450000 %\n",
      "training @ iter =  13000\n",
      "0.80437284708\n",
      "training @ iter =  13100\n",
      "0.684618532658\n",
      "training @ iter =  13200\n",
      "0.721642315388\n",
      "training @ iter =  13300\n",
      "0.688089966774\n",
      "training @ iter =  13400\n",
      "0.684748291969\n",
      "epoch 27, minibatch 500/500, test error 21.600000 %\n",
      "training @ iter =  13500\n",
      "0.654063463211\n",
      "training @ iter =  13600\n",
      "0.720065891743\n",
      "training @ iter =  13700\n",
      "0.702970564365\n",
      "training @ iter =  13800\n",
      "0.752677381039\n",
      "training @ iter =  13900\n",
      "0.583047032356\n",
      "epoch 28, minibatch 500/500, test error 22.940000 %\n",
      "training @ iter =  14000\n",
      "0.792750716209\n",
      "training @ iter =  14100\n",
      "0.657939016819\n",
      "training @ iter =  14200\n",
      "0.795732438564\n",
      "training @ iter =  14300\n",
      "0.766951441765\n",
      "training @ iter =  14400\n",
      "0.64648681879\n",
      "epoch 29, minibatch 500/500, test error 21.280000 %\n",
      "training @ iter =  14500\n",
      "0.626913428307\n",
      "training @ iter =  14600\n",
      "0.664980769157\n",
      "training @ iter =  14700\n",
      "0.655036568642\n",
      "training @ iter =  14800\n",
      "0.711468577385\n",
      "training @ iter =  14900\n",
      "0.58277618885\n",
      "epoch 30, minibatch 500/500, test error 20.640000 %\n",
      "training @ iter =  15000\n",
      "0.70282292366\n",
      "training @ iter =  15100\n",
      "0.607995569706\n",
      "training @ iter =  15200\n",
      "0.761400699615\n",
      "training @ iter =  15300\n",
      "0.698535919189\n",
      "training @ iter =  15400\n",
      "0.618904590607\n",
      "epoch 31, minibatch 500/500, test error 21.720000 %\n",
      "training @ iter =  15500\n",
      "0.666370868683\n",
      "training @ iter =  15600\n",
      "0.67899030447\n",
      "training @ iter =  15700\n",
      "0.742604315281\n",
      "training @ iter =  15800\n",
      "0.77781188488\n",
      "training @ iter =  15900\n",
      "0.555360257626\n",
      "epoch 32, minibatch 500/500, test error 21.060000 %\n",
      "training @ iter =  16000\n",
      "0.601458072662\n",
      "training @ iter =  16100\n",
      "0.718556344509\n",
      "training @ iter =  16200\n",
      "0.692770242691\n",
      "training @ iter =  16300\n",
      "0.653766155243\n",
      "training @ iter =  16400\n",
      "0.652867496014\n",
      "epoch 33, minibatch 500/500, test error 20.530000 %\n",
      "training @ iter =  16500\n",
      "0.674501895905\n",
      "training @ iter =  16600\n",
      "0.624000489712\n",
      "training @ iter =  16700\n",
      "0.553105711937\n",
      "training @ iter =  16800\n",
      "0.597485125065\n",
      "training @ iter =  16900\n",
      "0.510087192059\n",
      "epoch 34, minibatch 500/500, test error 21.060000 %\n",
      "training @ iter =  17000\n",
      "0.665302813053\n",
      "training @ iter =  17100\n",
      "0.596413731575\n",
      "training @ iter =  17200\n",
      "0.644074261189\n",
      "training @ iter =  17300\n",
      "0.715189039707\n",
      "training @ iter =  17400\n",
      "0.574834048748\n",
      "epoch 35, minibatch 500/500, test error 20.380000 %\n",
      "training @ iter =  17500\n",
      "0.675838649273\n",
      "training @ iter =  17600\n",
      "0.680075526237\n",
      "training @ iter =  17700\n",
      "0.661340653896\n",
      "training @ iter =  17800\n",
      "0.629662930965\n",
      "training @ iter =  17900\n",
      "0.417419940233\n",
      "epoch 36, minibatch 500/500, test error 21.240000 %\n",
      "training @ iter =  18000\n",
      "0.753193199635\n",
      "training @ iter =  18100\n",
      "0.744081258774\n",
      "training @ iter =  18200\n",
      "0.751885056496\n",
      "training @ iter =  18300\n",
      "0.677400767803\n",
      "training @ iter =  18400\n",
      "0.609615087509\n",
      "epoch 37, minibatch 500/500, test error 22.080000 %\n",
      "training @ iter =  18500\n",
      "0.651924431324\n",
      "training @ iter =  18600\n",
      "0.669853568077\n",
      "training @ iter =  18700\n",
      "0.682227790356\n",
      "training @ iter =  18800\n",
      "0.707340717316\n",
      "training @ iter =  18900\n",
      "0.545574426651\n",
      "epoch 38, minibatch 500/500, test error 19.780000 %\n",
      "training @ iter =  19000\n",
      "0.607241988182\n",
      "training @ iter =  19100\n",
      "0.656782925129\n",
      "training @ iter =  19200\n",
      "0.67353105545\n",
      "training @ iter =  19300\n",
      "0.608980774879\n",
      "training @ iter =  19400\n",
      "0.570445895195\n",
      "epoch 39, minibatch 500/500, test error 22.040000 %\n",
      "training @ iter =  19500\n",
      "0.685705423355\n",
      "training @ iter =  19600\n",
      "0.66393083334\n",
      "training @ iter =  19700\n",
      "0.782489061356\n",
      "training @ iter =  19800\n",
      "0.708836734295\n",
      "training @ iter =  19900\n",
      "0.649251878262\n",
      "epoch 40, minibatch 500/500, test error 21.070000 %\n",
      "training @ iter =  20000\n",
      "0.653272867203\n",
      "training @ iter =  20100\n",
      "0.606663942337\n",
      "training @ iter =  20200\n",
      "0.766106545925\n",
      "training @ iter =  20300\n",
      "0.610294401646\n",
      "training @ iter =  20400\n",
      "0.465117871761\n",
      "epoch 41, minibatch 500/500, test error 20.820000 %\n",
      "training @ iter =  20500\n",
      "0.770803391933\n",
      "training @ iter =  20600\n",
      "0.607450723648\n",
      "training @ iter =  20700\n",
      "0.690376460552\n",
      "training @ iter =  20800\n",
      "0.697653114796\n",
      "training @ iter =  20900\n",
      "0.562284469604\n",
      "epoch 42, minibatch 500/500, test error 21.010000 %\n",
      "training @ iter =  21000\n",
      "0.548320055008\n",
      "training @ iter =  21100\n",
      "0.666029155254\n",
      "training @ iter =  21200\n",
      "0.73871076107\n",
      "training @ iter =  21300\n",
      "0.709115743637\n",
      "training @ iter =  21400\n",
      "0.627221345901\n",
      "epoch 43, minibatch 500/500, test error 20.600000 %\n",
      "training @ iter =  21500\n",
      "0.571791887283\n",
      "training @ iter =  21600\n",
      "0.545850038528\n",
      "training @ iter =  21700\n",
      "0.699546635151\n",
      "training @ iter =  21800\n",
      "0.794124007225\n",
      "training @ iter =  21900\n",
      "0.701656520367\n",
      "epoch 44, minibatch 500/500, test error 20.960000 %\n",
      "training @ iter =  22000\n",
      "0.696416378021\n",
      "training @ iter =  22100\n",
      "0.623355388641\n",
      "training @ iter =  22200\n",
      "0.683121502399\n",
      "training @ iter =  22300\n",
      "0.641015768051\n",
      "training @ iter =  22400\n",
      "0.669099628925\n",
      "epoch 45, minibatch 500/500, test error 19.800000 %\n",
      "training @ iter =  22500\n",
      "0.664729356766\n",
      "training @ iter =  22600\n",
      "0.610597789288\n",
      "training @ iter =  22700\n",
      "0.704060792923\n",
      "training @ iter =  22800\n",
      "0.648449122906\n",
      "training @ iter =  22900\n",
      "0.59729629755\n",
      "epoch 46, minibatch 500/500, test error 21.080000 %\n",
      "training @ iter =  23000\n",
      "0.743501126766\n",
      "training @ iter =  23100\n",
      "0.629484593868\n",
      "training @ iter =  23200\n",
      "0.716628849506\n",
      "training @ iter =  23300\n",
      "0.627691745758\n",
      "training @ iter =  23400\n",
      "0.466488420963\n",
      "epoch 47, minibatch 500/500, test error 20.290000 %\n",
      "training @ iter =  23500\n",
      "0.61427038908\n",
      "training @ iter =  23600\n",
      "0.607435464859\n",
      "training @ iter =  23700\n",
      "0.563961803913\n",
      "training @ iter =  23800\n",
      "0.712904810905\n",
      "training @ iter =  23900\n",
      "0.512588143349\n",
      "epoch 48, minibatch 500/500, test error 20.000000 %\n",
      "training @ iter =  24000\n",
      "0.567902207375\n",
      "training @ iter =  24100\n",
      "0.53013330698\n",
      "training @ iter =  24200\n",
      "0.540770053864\n",
      "training @ iter =  24300\n",
      "0.712687969208\n",
      "training @ iter =  24400\n",
      "0.633584260941\n",
      "epoch 49, minibatch 500/500, test error 20.220000 %\n",
      "training @ iter =  24500\n",
      "0.722291409969\n",
      "training @ iter =  24600\n",
      "0.48635751009\n",
      "training @ iter =  24700\n",
      "0.658491432667\n",
      "training @ iter =  24800\n",
      "0.840733766556\n",
      "training @ iter =  24900\n",
      "0.485562086105\n",
      "epoch 50, minibatch 500/500, test error 20.370000 %\n",
      "training @ iter =  25000\n",
      "0.701202690601\n",
      "training @ iter =  25100\n",
      "0.64517647028\n",
      "training @ iter =  25200\n",
      "0.656906843185\n",
      "training @ iter =  25300\n",
      "0.612423300743\n",
      "training @ iter =  25400\n",
      "0.620864629745\n",
      "epoch 51, minibatch 500/500, test error 21.020000 %\n",
      "training @ iter =  25500\n",
      "0.68312227726\n",
      "training @ iter =  25600\n",
      "0.710417985916\n",
      "training @ iter =  25700\n",
      "0.635067462921\n",
      "training @ iter =  25800\n",
      "0.608128488064\n",
      "training @ iter =  25900\n",
      "0.671398699284\n",
      "epoch 52, minibatch 500/500, test error 20.440000 %\n",
      "training @ iter =  26000\n",
      "0.568793475628\n",
      "training @ iter =  26100\n",
      "0.565847635269\n",
      "training @ iter =  26200\n",
      "0.674924135208\n",
      "training @ iter =  26300\n",
      "0.605623006821\n",
      "training @ iter =  26400\n",
      "0.585697472095\n",
      "epoch 53, minibatch 500/500, test error 19.670000 %\n",
      "training @ iter =  26500\n",
      "0.540743172169\n",
      "training @ iter =  26600\n",
      "0.588998258114\n",
      "training @ iter =  26700\n",
      "0.5827870965\n",
      "training @ iter =  26800\n",
      "0.774715781212\n",
      "training @ iter =  26900\n",
      "0.432766318321\n",
      "epoch 54, minibatch 500/500, test error 20.330000 %\n",
      "training @ iter =  27000\n",
      "0.603702366352\n",
      "training @ iter =  27100\n",
      "0.801168978214\n",
      "training @ iter =  27200\n",
      "0.559362769127\n",
      "training @ iter =  27300\n",
      "0.534802794456\n",
      "training @ iter =  27400\n",
      "0.479107439518\n",
      "epoch 55, minibatch 500/500, test error 19.410000 %\n",
      "training @ iter =  27500\n",
      "0.598158478737\n",
      "training @ iter =  27600\n",
      "0.591349601746\n",
      "training @ iter =  27700\n",
      "0.552743852139\n",
      "training @ iter =  27800\n",
      "0.569897174835\n",
      "training @ iter =  27900\n",
      "0.657330036163\n",
      "epoch 56, minibatch 500/500, test error 19.830000 %\n",
      "training @ iter =  28000\n",
      "0.628705620766\n",
      "training @ iter =  28100\n",
      "0.642708837986\n",
      "training @ iter =  28200\n",
      "0.591027796268\n",
      "training @ iter =  28300\n",
      "0.608568668365\n",
      "training @ iter =  28400\n",
      "0.481044083834\n",
      "epoch 57, minibatch 500/500, test error 20.040000 %\n",
      "training @ iter =  28500\n",
      "0.654664635658\n",
      "training @ iter =  28600\n",
      "0.605835974216\n",
      "training @ iter =  28700\n",
      "0.826525807381\n",
      "training @ iter =  28800\n",
      "0.580775439739\n",
      "training @ iter =  28900\n",
      "0.612466037273\n",
      "epoch 58, minibatch 500/500, test error 19.720000 %\n",
      "training @ iter =  29000\n",
      "0.735471189022\n",
      "training @ iter =  29100\n",
      "0.709272027016\n",
      "training @ iter =  29200\n",
      "0.694799184799\n",
      "training @ iter =  29300\n",
      "0.652899682522\n",
      "training @ iter =  29400\n",
      "0.487026631832\n",
      "epoch 59, minibatch 500/500, test error 19.580000 %\n",
      "training @ iter =  29500\n",
      "0.63645106554\n",
      "training @ iter =  29600\n",
      "0.638611078262\n",
      "training @ iter =  29700\n",
      "0.65059953928\n",
      "training @ iter =  29800\n",
      "0.618203818798\n",
      "training @ iter =  29900\n",
      "0.443992376328\n",
      "epoch 60, minibatch 500/500, test error 19.750000 %\n",
      "training @ iter =  30000\n",
      "0.668874800205\n",
      "training @ iter =  30100\n",
      "0.621032118797\n",
      "training @ iter =  30200\n",
      "0.634466528893\n",
      "training @ iter =  30300\n",
      "0.659445345402\n",
      "training @ iter =  30400\n",
      "0.645560026169\n",
      "epoch 61, minibatch 500/500, test error 19.370000 %\n",
      "training @ iter =  30500\n",
      "0.564375281334\n",
      "training @ iter =  30600\n",
      "0.512909829617\n",
      "training @ iter =  30700\n",
      "0.450013279915\n",
      "training @ iter =  30800\n",
      "0.719586551189\n",
      "training @ iter =  30900\n",
      "0.506042838097\n",
      "epoch 62, minibatch 500/500, test error 20.610000 %\n",
      "training @ iter =  31000\n",
      "0.671974003315\n",
      "training @ iter =  31100\n",
      "0.609052479267\n",
      "training @ iter =  31200\n",
      "0.648420870304\n",
      "training @ iter =  31300\n",
      "0.569274306297\n",
      "training @ iter =  31400\n",
      "0.444650262594\n",
      "epoch 63, minibatch 500/500, test error 21.060000 %\n",
      "training @ iter =  31500\n",
      "0.636763691902\n",
      "training @ iter =  31600\n",
      "0.655793607235\n",
      "training @ iter =  31700\n",
      "0.461512386799\n",
      "training @ iter =  31800\n",
      "0.804348468781\n",
      "training @ iter =  31900\n",
      "0.558586299419\n",
      "epoch 64, minibatch 500/500, test error 19.460000 %\n",
      "training @ iter =  32000\n",
      "0.834977865219\n",
      "training @ iter =  32100\n",
      "0.538844168186\n",
      "training @ iter =  32200\n",
      "0.566201984882\n",
      "training @ iter =  32300\n",
      "0.676848769188\n",
      "training @ iter =  32400\n",
      "0.534377038479\n",
      "epoch 65, minibatch 500/500, test error 20.560000 %\n",
      "training @ iter =  32500\n",
      "0.628347098827\n",
      "training @ iter =  32600\n",
      "0.543421030045\n",
      "training @ iter =  32700\n",
      "0.462235569954\n",
      "training @ iter =  32800\n",
      "0.623701691628\n",
      "training @ iter =  32900\n",
      "0.542225062847\n",
      "epoch 66, minibatch 500/500, test error 20.960000 %\n",
      "training @ iter =  33000\n",
      "0.683677613735\n",
      "training @ iter =  33100\n",
      "0.646381020546\n",
      "training @ iter =  33200\n",
      "0.663017272949\n",
      "training @ iter =  33300\n",
      "0.786369919777\n",
      "training @ iter =  33400\n",
      "0.601729869843\n",
      "epoch 67, minibatch 500/500, test error 20.020000 %\n",
      "training @ iter =  33500\n",
      "0.64041185379\n",
      "training @ iter =  33600\n",
      "0.715427100658\n",
      "training @ iter =  33700\n",
      "0.598409175873\n",
      "training @ iter =  33800\n",
      "0.637033879757\n",
      "training @ iter =  33900\n",
      "0.511451542377\n",
      "epoch 68, minibatch 500/500, test error 19.210000 %\n",
      "training @ iter =  34000\n",
      "0.692466437817\n",
      "training @ iter =  34100\n",
      "0.542502760887\n",
      "training @ iter =  34200\n",
      "0.693759739399\n",
      "training @ iter =  34300\n",
      "0.626746714115\n",
      "training @ iter =  34400\n",
      "0.48262450099\n",
      "epoch 69, minibatch 500/500, test error 19.110000 %\n",
      "training @ iter =  34500\n",
      "0.614144682884\n",
      "training @ iter =  34600\n",
      "0.518185853958\n",
      "training @ iter =  34700\n",
      "0.634455859661\n",
      "training @ iter =  34800\n",
      "0.768522977829\n",
      "training @ iter =  34900\n",
      "0.66830343008\n",
      "epoch 70, minibatch 500/500, test error 20.520000 %\n",
      "training @ iter =  35000\n",
      "0.734658658504\n",
      "training @ iter =  35100\n",
      "0.599691212177\n",
      "training @ iter =  35200\n",
      "0.642390906811\n",
      "training @ iter =  35300\n",
      "0.552834749222\n",
      "training @ iter =  35400\n",
      "0.431759119034\n",
      "epoch 71, minibatch 500/500, test error 20.170000 %\n",
      "training @ iter =  35500\n",
      "0.639432966709\n",
      "training @ iter =  35600\n",
      "0.576360344887\n",
      "training @ iter =  35700\n",
      "0.559401571751\n",
      "training @ iter =  35800\n",
      "0.587302267551\n",
      "training @ iter =  35900\n",
      "0.509481668472\n",
      "epoch 72, minibatch 500/500, test error 19.660000 %\n",
      "training @ iter =  36000\n",
      "0.571951389313\n",
      "training @ iter =  36100\n",
      "0.516718268394\n",
      "training @ iter =  36200\n",
      "0.697222650051\n",
      "training @ iter =  36300\n",
      "0.643871605396\n",
      "training @ iter =  36400\n",
      "0.457990258932\n",
      "epoch 73, minibatch 500/500, test error 20.820000 %\n",
      "training @ iter =  36500\n",
      "0.700043201447\n",
      "training @ iter =  36600\n",
      "0.720500946045\n",
      "training @ iter =  36700\n",
      "0.684277355671\n",
      "training @ iter =  36800\n",
      "0.707804620266\n",
      "training @ iter =  36900\n",
      "0.536011099815\n",
      "epoch 74, minibatch 500/500, test error 19.740000 %\n",
      "training @ iter =  37000\n",
      "0.612661123276\n",
      "training @ iter =  37100\n",
      "0.761710047722\n",
      "training @ iter =  37200\n",
      "0.698708951473\n",
      "training @ iter =  37300\n",
      "0.635880410671\n",
      "training @ iter =  37400\n",
      "0.492059201002\n",
      "epoch 75, minibatch 500/500, test error 19.530000 %\n",
      "training @ iter =  37500\n",
      "0.737767636776\n",
      "training @ iter =  37600\n",
      "0.776463150978\n",
      "training @ iter =  37700\n",
      "0.621572375298\n",
      "training @ iter =  37800\n",
      "0.62539935112\n",
      "training @ iter =  37900\n",
      "0.509029626846\n",
      "epoch 76, minibatch 500/500, test error 20.000000 %\n",
      "training @ iter =  38000\n",
      "0.573325276375\n",
      "training @ iter =  38100\n",
      "0.53381562233\n",
      "training @ iter =  38200\n",
      "0.701050579548\n",
      "training @ iter =  38300\n",
      "0.662800848484\n",
      "training @ iter =  38400\n",
      "0.559908926487\n",
      "epoch 77, minibatch 500/500, test error 19.130000 %\n",
      "training @ iter =  38500\n",
      "0.757502913475\n",
      "training @ iter =  38600\n",
      "0.513819038868\n",
      "training @ iter =  38700\n",
      "0.542312145233\n",
      "training @ iter =  38800\n",
      "0.710570991039\n",
      "training @ iter =  38900\n",
      "0.584047973156\n",
      "epoch 78, minibatch 500/500, test error 19.620000 %\n",
      "training @ iter =  39000\n",
      "0.50750374794\n",
      "training @ iter =  39100\n",
      "0.570097208023\n",
      "training @ iter =  39200\n",
      "0.652142643929\n",
      "training @ iter =  39300\n",
      "0.70537352562\n",
      "training @ iter =  39400\n",
      "0.787229001522\n",
      "epoch 79, minibatch 500/500, test error 19.520000 %\n",
      "training @ iter =  39500\n",
      "0.577224969864\n",
      "training @ iter =  39600\n",
      "0.556067287922\n",
      "training @ iter =  39700\n",
      "0.589209735394\n",
      "training @ iter =  39800\n",
      "0.589708149433\n",
      "training @ iter =  39900\n",
      "0.426409572363\n",
      "epoch 80, minibatch 500/500, test error 19.300000 %\n",
      "training @ iter =  40000\n",
      "0.539347290993\n",
      "training @ iter =  40100\n",
      "0.520651102066\n",
      "training @ iter =  40200\n",
      "0.630310058594\n",
      "training @ iter =  40300\n",
      "0.578625321388\n",
      "training @ iter =  40400\n",
      "0.499797135592\n",
      "epoch 81, minibatch 500/500, test error 20.700000 %\n",
      "training @ iter =  40500\n",
      "0.702557384968\n",
      "training @ iter =  40600\n",
      "0.477418512106\n",
      "training @ iter =  40700\n",
      "0.61150097847\n",
      "training @ iter =  40800\n",
      "0.525678932667\n",
      "training @ iter =  40900\n",
      "0.494825512171\n",
      "epoch 82, minibatch 500/500, test error 19.460000 %\n",
      "training @ iter =  41000\n",
      "0.600855231285\n",
      "training @ iter =  41100\n",
      "0.561012625694\n",
      "training @ iter =  41200\n",
      "0.560213029385\n",
      "training @ iter =  41300\n",
      "0.84325760603\n",
      "training @ iter =  41400\n",
      "0.59743976593\n",
      "epoch 83, minibatch 500/500, test error 19.860000 %\n",
      "training @ iter =  41500\n",
      "0.681824505329\n",
      "training @ iter =  41600\n",
      "0.58934032917\n",
      "training @ iter =  41700\n",
      "0.525866091251\n",
      "training @ iter =  41800\n",
      "0.634231209755\n",
      "training @ iter =  41900\n",
      "0.469843029976\n",
      "epoch 84, minibatch 500/500, test error 19.330000 %\n",
      "training @ iter =  42000\n",
      "0.581674098969\n",
      "training @ iter =  42100\n",
      "0.558870851994\n",
      "training @ iter =  42200\n",
      "0.743930995464\n",
      "training @ iter =  42300\n",
      "0.730807065964\n",
      "training @ iter =  42400\n",
      "0.513521552086\n",
      "epoch 85, minibatch 500/500, test error 20.450000 %\n",
      "training @ iter =  42500\n",
      "0.630616366863\n",
      "training @ iter =  42600\n",
      "0.59648925066\n",
      "training @ iter =  42700\n",
      "0.506754040718\n",
      "training @ iter =  42800\n",
      "0.580399990082\n",
      "training @ iter =  42900\n",
      "0.597184360027\n",
      "epoch 86, minibatch 500/500, test error 19.760000 %\n",
      "training @ iter =  43000\n",
      "0.631771743298\n",
      "training @ iter =  43100\n",
      "0.464436262846\n",
      "training @ iter =  43200\n",
      "0.669622123241\n",
      "training @ iter =  43300\n",
      "0.619518339634\n",
      "training @ iter =  43400\n",
      "0.588304340839\n",
      "epoch 87, minibatch 500/500, test error 20.450000 %\n",
      "training @ iter =  43500\n",
      "0.515458464622\n",
      "training @ iter =  43600\n",
      "0.540901958942\n",
      "training @ iter =  43700\n",
      "0.580649733543\n",
      "training @ iter =  43800\n",
      "0.570995807648\n",
      "training @ iter =  43900\n",
      "0.598762512207\n",
      "epoch 88, minibatch 500/500, test error 19.290000 %\n",
      "training @ iter =  44000\n",
      "0.681295394897\n",
      "training @ iter =  44100\n",
      "0.650605916977\n",
      "training @ iter =  44200\n",
      "0.561594069004\n",
      "training @ iter =  44300\n",
      "0.690483927727\n",
      "training @ iter =  44400\n",
      "0.518356084824\n",
      "epoch 89, minibatch 500/500, test error 19.720000 %\n",
      "training @ iter =  44500\n",
      "0.544370293617\n",
      "training @ iter =  44600\n",
      "0.830507874489\n",
      "training @ iter =  44700\n",
      "0.513108551502\n",
      "training @ iter =  44800\n",
      "0.646476566792\n",
      "training @ iter =  44900\n",
      "0.574753522873\n",
      "epoch 90, minibatch 500/500, test error 20.220000 %\n",
      "training @ iter =  45000\n",
      "0.68000292778\n",
      "training @ iter =  45100\n",
      "0.681600809097\n",
      "training @ iter =  45200\n",
      "0.551051259041\n",
      "training @ iter =  45300\n",
      "0.759152770042\n",
      "training @ iter =  45400\n",
      "0.4451161623\n",
      "epoch 91, minibatch 500/500, test error 20.990000 %\n",
      "training @ iter =  45500\n",
      "0.748162090778\n",
      "training @ iter =  45600\n",
      "0.580395638943\n",
      "training @ iter =  45700\n",
      "0.533769726753\n",
      "training @ iter =  45800\n",
      "0.504914224148\n",
      "training @ iter =  45900\n",
      "0.578018248081\n",
      "epoch 92, minibatch 500/500, test error 20.280000 %\n",
      "training @ iter =  46000\n",
      "0.510924756527\n",
      "training @ iter =  46100\n",
      "0.635574281216\n",
      "training @ iter =  46200\n",
      "0.582748830318\n",
      "training @ iter =  46300\n",
      "0.661477029324\n",
      "training @ iter =  46400\n",
      "0.513131558895\n",
      "epoch 93, minibatch 500/500, test error 19.790000 %\n",
      "training @ iter =  46500\n",
      "0.691971004009\n",
      "training @ iter =  46600\n",
      "0.523479700089\n",
      "training @ iter =  46700\n",
      "0.549716711044\n",
      "training @ iter =  46800\n",
      "0.561323404312\n",
      "training @ iter =  46900\n",
      "0.640864193439\n",
      "epoch 94, minibatch 500/500, test error 19.160000 %\n",
      "training @ iter =  47000\n",
      "0.611648797989\n",
      "training @ iter =  47100\n",
      "0.555868327618\n",
      "training @ iter =  47200\n",
      "0.528070628643\n",
      "training @ iter =  47300\n",
      "0.63245511055\n",
      "training @ iter =  47400\n",
      "0.622600853443\n",
      "epoch 95, minibatch 500/500, test error 19.380000 %\n",
      "training @ iter =  47500\n",
      "0.714274764061\n",
      "training @ iter =  47600\n",
      "0.722951173782\n",
      "training @ iter =  47700\n",
      "0.477931141853\n",
      "training @ iter =  47800\n",
      "0.496479719877\n",
      "training @ iter =  47900\n",
      "0.538636147976\n",
      "epoch 96, minibatch 500/500, test error 19.670000 %\n",
      "training @ iter =  48000\n",
      "0.714220404625\n",
      "training @ iter =  48100\n",
      "0.609648406506\n",
      "training @ iter =  48200\n",
      "0.564117968082\n",
      "training @ iter =  48300\n",
      "0.651758670807\n",
      "training @ iter =  48400\n",
      "0.454987943172\n",
      "epoch 97, minibatch 500/500, test error 20.720000 %\n",
      "training @ iter =  48500\n",
      "0.703471839428\n",
      "training @ iter =  48600\n",
      "0.53004193306\n",
      "training @ iter =  48700\n",
      "0.571386516094\n",
      "training @ iter =  48800\n",
      "0.609398245811\n",
      "training @ iter =  48900\n",
      "0.613391578197\n",
      "epoch 98, minibatch 500/500, test error 20.940000 %\n",
      "training @ iter =  49000\n",
      "0.623944163322\n",
      "training @ iter =  49100\n",
      "0.722239673138\n",
      "training @ iter =  49200\n",
      "0.603771924973\n",
      "training @ iter =  49300\n",
      "0.684251964092\n",
      "training @ iter =  49400\n",
      "0.571152746677\n",
      "epoch 99, minibatch 500/500, test error 19.860000 %\n",
      "training @ iter =  49500\n",
      "0.62138992548\n",
      "training @ iter =  49600\n",
      "0.659375011921\n",
      "training @ iter =  49700\n",
      "0.556793391705\n",
      "training @ iter =  49800\n",
      "0.632061243057\n",
      "training @ iter =  49900\n",
      "0.448406338692\n",
      "epoch 100, minibatch 500/500, test error 19.170000 %\n",
      "training @ iter =  50000\n",
      "0.581830739975\n",
      "training @ iter =  50100\n",
      "0.618943333626\n",
      "training @ iter =  50200\n",
      "0.662906169891\n",
      "training @ iter =  50300\n",
      "0.574836432934\n",
      "training @ iter =  50400\n",
      "0.546153783798\n",
      "epoch 101, minibatch 500/500, test error 19.450000 %\n",
      "training @ iter =  50500\n",
      "0.657238781452\n",
      "training @ iter =  50600\n",
      "0.584766030312\n",
      "training @ iter =  50700\n",
      "0.669384777546\n",
      "training @ iter =  50800\n",
      "0.559954464436\n",
      "training @ iter =  50900\n",
      "0.456136792898\n",
      "epoch 102, minibatch 500/500, test error 19.410000 %\n",
      "training @ iter =  51000\n",
      "0.591613709927\n",
      "training @ iter =  51100\n",
      "0.501540839672\n",
      "training @ iter =  51200\n",
      "0.481460958719\n",
      "training @ iter =  51300\n",
      "0.667758762836\n",
      "training @ iter =  51400\n",
      "0.595862388611\n",
      "epoch 103, minibatch 500/500, test error 19.340000 %\n",
      "training @ iter =  51500\n",
      "0.633538663387\n",
      "training @ iter =  51600\n",
      "0.617932975292\n",
      "training @ iter =  51700\n",
      "0.526094257832\n",
      "training @ iter =  51800\n",
      "0.619289219379\n",
      "training @ iter =  51900\n",
      "0.450455158949\n",
      "epoch 104, minibatch 500/500, test error 20.490000 %\n",
      "training @ iter =  52000\n",
      "0.733737468719\n",
      "training @ iter =  52100\n",
      "0.541129946709\n",
      "training @ iter =  52200\n",
      "0.571689009666\n",
      "training @ iter =  52300\n",
      "0.654850482941\n",
      "training @ iter =  52400\n",
      "0.490593910217\n",
      "epoch 105, minibatch 500/500, test error 19.440000 %\n",
      "training @ iter =  52500\n",
      "0.727422654629\n",
      "training @ iter =  52600\n",
      "0.59161055088\n",
      "training @ iter =  52700\n",
      "0.683857560158\n",
      "training @ iter =  52800\n",
      "0.659761965275\n",
      "training @ iter =  52900\n",
      "0.553158819675\n",
      "epoch 106, minibatch 500/500, test error 19.040000 %\n",
      "training @ iter =  53000\n",
      "0.663157522678\n",
      "training @ iter =  53100\n",
      "0.664825201035\n",
      "training @ iter =  53200\n",
      "0.747460007668\n",
      "training @ iter =  53300\n",
      "0.60860055685\n",
      "training @ iter =  53400\n",
      "0.534857809544\n",
      "epoch 107, minibatch 500/500, test error 18.490000 %\n",
      "training @ iter =  53500\n",
      "0.526781439781\n",
      "training @ iter =  53600\n",
      "0.72825807333\n",
      "training @ iter =  53700\n",
      "0.565079808235\n",
      "training @ iter =  53800\n",
      "0.693735480309\n",
      "training @ iter =  53900\n",
      "0.541391551495\n",
      "epoch 108, minibatch 500/500, test error 19.930000 %\n",
      "training @ iter =  54000\n",
      "0.599847555161\n",
      "training @ iter =  54100\n",
      "0.503783881664\n",
      "training @ iter =  54200\n",
      "0.614446401596\n",
      "training @ iter =  54300\n",
      "0.650363028049\n",
      "training @ iter =  54400\n",
      "0.592619717121\n",
      "epoch 109, minibatch 500/500, test error 19.330000 %\n",
      "training @ iter =  54500\n",
      "0.700898885727\n",
      "training @ iter =  54600\n",
      "0.663435041904\n",
      "training @ iter =  54700\n",
      "0.606645107269\n",
      "training @ iter =  54800\n",
      "0.765507876873\n",
      "training @ iter =  54900\n",
      "0.589460670948\n",
      "epoch 110, minibatch 500/500, test error 19.220000 %\n",
      "training @ iter =  55000\n",
      "0.644326865673\n",
      "training @ iter =  55100\n",
      "0.516064286232\n",
      "training @ iter =  55200\n",
      "0.548861861229\n",
      "training @ iter =  55300\n",
      "0.684706568718\n",
      "training @ iter =  55400\n",
      "0.582434535027\n",
      "epoch 111, minibatch 500/500, test error 19.440000 %\n",
      "training @ iter =  55500\n",
      "0.757359921932\n",
      "training @ iter =  55600\n",
      "0.645814836025\n",
      "training @ iter =  55700\n",
      "0.682015061378\n",
      "training @ iter =  55800\n",
      "0.611642122269\n",
      "training @ iter =  55900\n",
      "0.509139835835\n",
      "epoch 112, minibatch 500/500, test error 20.450000 %\n",
      "training @ iter =  56000\n",
      "0.777190387249\n",
      "training @ iter =  56100\n",
      "0.612695395947\n",
      "training @ iter =  56200\n",
      "0.548942089081\n",
      "training @ iter =  56300\n",
      "0.605126261711\n",
      "training @ iter =  56400\n",
      "0.498756557703\n",
      "epoch 113, minibatch 500/500, test error 19.910000 %\n",
      "training @ iter =  56500\n",
      "0.722657442093\n",
      "training @ iter =  56600\n",
      "0.634641170502\n",
      "training @ iter =  56700\n",
      "0.556439876556\n",
      "training @ iter =  56800\n",
      "0.749609291553\n",
      "training @ iter =  56900\n",
      "0.660683274269\n",
      "epoch 114, minibatch 500/500, test error 21.920000 %\n",
      "training @ iter =  57000\n",
      "0.67529129982\n",
      "training @ iter =  57100\n",
      "0.544630646706\n",
      "training @ iter =  57200\n",
      "0.517230093479\n",
      "training @ iter =  57300\n",
      "0.760799229145\n",
      "training @ iter =  57400\n",
      "0.547385156155\n",
      "epoch 115, minibatch 500/500, test error 20.720000 %\n",
      "training @ iter =  57500\n",
      "0.611596882343\n",
      "training @ iter =  57600\n",
      "0.626326322556\n",
      "training @ iter =  57700\n",
      "0.568500041962\n",
      "training @ iter =  57800\n",
      "0.698492884636\n",
      "training @ iter =  57900\n",
      "0.510966956615\n",
      "epoch 116, minibatch 500/500, test error 19.460000 %\n",
      "training @ iter =  58000\n",
      "0.625783443451\n",
      "training @ iter =  58100\n",
      "0.546073317528\n",
      "training @ iter =  58200\n",
      "0.622043251991\n",
      "training @ iter =  58300\n",
      "0.745699286461\n",
      "training @ iter =  58400\n",
      "0.591106593609\n",
      "epoch 117, minibatch 500/500, test error 19.340000 %\n",
      "training @ iter =  58500\n",
      "0.68459379673\n",
      "training @ iter =  58600\n",
      "0.633280038834\n",
      "training @ iter =  58700\n",
      "0.553411245346\n",
      "training @ iter =  58800\n",
      "0.672302544117\n",
      "training @ iter =  58900\n",
      "0.568177700043\n",
      "epoch 118, minibatch 500/500, test error 19.610000 %\n",
      "training @ iter =  59000\n",
      "0.624373912811\n",
      "training @ iter =  59100\n",
      "0.587029755116\n",
      "training @ iter =  59200\n",
      "0.618145227432\n",
      "training @ iter =  59300\n",
      "0.642945468426\n",
      "training @ iter =  59400\n",
      "0.50153028965\n",
      "epoch 119, minibatch 500/500, test error 19.350000 %\n",
      "training @ iter =  59500\n",
      "0.628644168377\n",
      "training @ iter =  59600\n",
      "0.578351199627\n",
      "training @ iter =  59700\n",
      "0.537217080593\n",
      "training @ iter =  59800\n",
      "0.746514201164\n",
      "training @ iter =  59900\n",
      "0.588429629803\n",
      "epoch 120, minibatch 500/500, test error 19.450000 %\n",
      "training @ iter =  60000\n",
      "0.71399217844\n",
      "training @ iter =  60100\n",
      "0.54520624876\n",
      "training @ iter =  60200\n",
      "0.556145250797\n",
      "training @ iter =  60300\n",
      "0.704251110554\n",
      "training @ iter =  60400\n",
      "0.434290766716\n",
      "epoch 121, minibatch 500/500, test error 20.790000 %\n",
      "training @ iter =  60500\n",
      "0.583183705807\n",
      "training @ iter =  60600\n",
      "0.724899590015\n",
      "training @ iter =  60700\n",
      "0.685290813446\n",
      "training @ iter =  60800\n",
      "0.733461737633\n",
      "training @ iter =  60900\n",
      "0.634664595127\n",
      "epoch 122, minibatch 500/500, test error 18.930000 %\n",
      "training @ iter =  61000\n",
      "0.520285785198\n",
      "training @ iter =  61100\n",
      "0.542328238487\n",
      "training @ iter =  61200\n",
      "0.673159122467\n",
      "training @ iter =  61300\n",
      "0.679109871387\n",
      "training @ iter =  61400\n",
      "0.553282856941\n",
      "epoch 123, minibatch 500/500, test error 20.200000 %\n",
      "training @ iter =  61500\n",
      "0.597670614719\n",
      "training @ iter =  61600\n",
      "0.632008373737\n",
      "training @ iter =  61700\n",
      "0.6272149086\n",
      "training @ iter =  61800\n",
      "0.766751050949\n",
      "training @ iter =  61900\n",
      "0.620603561401\n",
      "epoch 124, minibatch 500/500, test error 19.300000 %\n",
      "training @ iter =  62000\n",
      "0.516495347023\n",
      "training @ iter =  62100\n",
      "0.79284286499\n",
      "training @ iter =  62200\n",
      "0.573169529438\n",
      "training @ iter =  62300\n",
      "0.70848107338\n",
      "training @ iter =  62400\n",
      "0.579864561558\n",
      "epoch 125, minibatch 500/500, test error 19.810000 %\n",
      "training @ iter =  62500\n",
      "0.609759032726\n",
      "training @ iter =  62600\n",
      "0.637842595577\n",
      "training @ iter =  62700\n",
      "0.576986610889\n",
      "training @ iter =  62800\n",
      "0.549845337868\n",
      "training @ iter =  62900\n",
      "0.627160847187\n",
      "epoch 126, minibatch 500/500, test error 19.970000 %\n",
      "training @ iter =  63000\n",
      "0.499164044857\n",
      "training @ iter =  63100\n",
      "0.532580018044\n",
      "training @ iter =  63200\n",
      "0.547152936459\n",
      "training @ iter =  63300\n",
      "0.636612772942\n",
      "training @ iter =  63400\n",
      "0.619693756104\n",
      "epoch 127, minibatch 500/500, test error 20.200000 %\n",
      "training @ iter =  63500\n",
      "0.766678154469\n",
      "training @ iter =  63600\n",
      "0.632652282715\n",
      "training @ iter =  63700\n",
      "0.588033020496\n",
      "training @ iter =  63800\n",
      "0.75218552351\n",
      "training @ iter =  63900\n",
      "0.633993685246\n",
      "epoch 128, minibatch 500/500, test error 20.150000 %\n",
      "training @ iter =  64000\n",
      "0.511583924294\n",
      "training @ iter =  64100\n",
      "0.640233695507\n",
      "training @ iter =  64200\n",
      "0.605216622353\n",
      "training @ iter =  64300\n",
      "0.603206276894\n",
      "training @ iter =  64400\n",
      "0.577286362648\n",
      "epoch 129, minibatch 500/500, test error 19.870000 %\n",
      "training @ iter =  64500\n",
      "0.590290307999\n",
      "training @ iter =  64600\n",
      "0.385085910559\n",
      "training @ iter =  64700\n",
      "0.675236046314\n",
      "training @ iter =  64800\n",
      "0.697471618652\n",
      "training @ iter =  64900\n",
      "0.751488029957\n",
      "epoch 130, minibatch 500/500, test error 20.190000 %\n",
      "training @ iter =  65000\n",
      "0.905395507812\n",
      "training @ iter =  65100\n",
      "0.618840038776\n",
      "training @ iter =  65200\n",
      "0.610444545746\n",
      "training @ iter =  65300\n",
      "0.740931272507\n",
      "training @ iter =  65400\n",
      "0.394976645708\n",
      "epoch 131, minibatch 500/500, test error 20.640000 %\n",
      "training @ iter =  65500\n",
      "0.776837170124\n",
      "training @ iter =  65600\n",
      "0.488541483879\n",
      "training @ iter =  65700\n",
      "0.653947114944\n",
      "training @ iter =  65800\n",
      "0.686144888401\n",
      "training @ iter =  65900\n",
      "0.523438870907\n",
      "epoch 132, minibatch 500/500, test error 18.990000 %\n",
      "training @ iter =  66000\n",
      "0.593610286713\n",
      "training @ iter =  66100\n",
      "0.562561333179\n",
      "training @ iter =  66200\n",
      "0.570007801056\n",
      "training @ iter =  66300\n",
      "0.649047136307\n",
      "training @ iter =  66400\n",
      "0.479959577322\n",
      "epoch 133, minibatch 500/500, test error 19.880000 %\n",
      "training @ iter =  66500\n",
      "0.641906261444\n",
      "training @ iter =  66600\n",
      "0.520107865334\n",
      "training @ iter =  66700\n",
      "0.660486757755\n",
      "training @ iter =  66800\n",
      "0.818083941936\n",
      "training @ iter =  66900\n",
      "0.53415620327\n",
      "epoch 134, minibatch 500/500, test error 19.640000 %\n",
      "training @ iter =  67000\n",
      "0.827326655388\n",
      "training @ iter =  67100\n",
      "0.564363002777\n",
      "training @ iter =  67200\n",
      "0.587404608727\n",
      "training @ iter =  67300\n",
      "0.511620402336\n",
      "training @ iter =  67400\n",
      "0.53904235363\n",
      "epoch 135, minibatch 500/500, test error 18.920000 %\n",
      "training @ iter =  67500\n",
      "0.637447118759\n",
      "training @ iter =  67600\n",
      "0.621660470963\n",
      "training @ iter =  67700\n",
      "0.720031499863\n",
      "training @ iter =  67800\n",
      "0.571604967117\n",
      "training @ iter =  67900\n",
      "0.480838924646\n",
      "epoch 136, minibatch 500/500, test error 20.140000 %\n",
      "training @ iter =  68000\n",
      "0.700718700886\n",
      "training @ iter =  68100\n",
      "0.703635275364\n",
      "training @ iter =  68200\n",
      "0.559401571751\n",
      "training @ iter =  68300\n",
      "0.56377774477\n",
      "training @ iter =  68400\n",
      "0.581651747227\n",
      "epoch 137, minibatch 500/500, test error 20.510000 %\n",
      "training @ iter =  68500\n",
      "0.613383769989\n",
      "training @ iter =  68600\n",
      "0.598066329956\n",
      "training @ iter =  68700\n",
      "0.703593850136\n",
      "training @ iter =  68800\n",
      "0.67276763916\n",
      "training @ iter =  68900\n",
      "0.473679184914\n",
      "epoch 138, minibatch 500/500, test error 20.130000 %\n",
      "training @ iter =  69000\n",
      "0.539422094822\n",
      "training @ iter =  69100\n",
      "0.551177442074\n",
      "training @ iter =  69200\n",
      "0.559205174446\n",
      "training @ iter =  69300\n",
      "0.560695827007\n",
      "training @ iter =  69400\n",
      "0.548730671406\n",
      "epoch 139, minibatch 500/500, test error 20.230000 %\n",
      "training @ iter =  69500\n",
      "0.544399499893\n",
      "training @ iter =  69600\n",
      "0.563209474087\n",
      "training @ iter =  69700\n",
      "0.643625497818\n",
      "training @ iter =  69800\n",
      "0.692627847195\n",
      "training @ iter =  69900\n",
      "0.569620490074\n",
      "epoch 140, minibatch 500/500, test error 19.670000 %\n",
      "training @ iter =  70000\n",
      "0.668147027493\n",
      "training @ iter =  70100\n",
      "0.616288423538\n",
      "training @ iter =  70200\n",
      "0.662659823895\n",
      "training @ iter =  70300\n",
      "0.671977698803\n",
      "training @ iter =  70400\n",
      "0.458495110273\n",
      "epoch 141, minibatch 500/500, test error 20.440000 %\n",
      "training @ iter =  70500\n",
      "0.535539925098\n",
      "training @ iter =  70600\n",
      "0.568914651871\n",
      "training @ iter =  70700\n",
      "0.718217015266\n",
      "training @ iter =  70800\n",
      "0.642800748348\n",
      "training @ iter =  70900\n",
      "0.560357272625\n",
      "epoch 142, minibatch 500/500, test error 19.360000 %\n",
      "training @ iter =  71000\n",
      "0.669670283794\n",
      "training @ iter =  71100\n",
      "0.699528813362\n",
      "training @ iter =  71200\n",
      "0.759600520134\n",
      "training @ iter =  71300\n",
      "0.656111478806\n",
      "training @ iter =  71400\n",
      "0.564536869526\n",
      "epoch 143, minibatch 500/500, test error 19.620000 %\n",
      "training @ iter =  71500\n",
      "0.727533578873\n",
      "training @ iter =  71600\n",
      "0.586596846581\n",
      "training @ iter =  71700\n",
      "0.648670434952\n",
      "training @ iter =  71800\n",
      "0.733760595322\n",
      "training @ iter =  71900\n",
      "0.53299498558\n",
      "epoch 144, minibatch 500/500, test error 19.110000 %\n",
      "training @ iter =  72000\n",
      "0.733351290226\n",
      "training @ iter =  72100\n",
      "0.549802720547\n",
      "training @ iter =  72200\n",
      "0.737014532089\n",
      "training @ iter =  72300\n",
      "0.693675100803\n",
      "training @ iter =  72400\n",
      "0.564402699471\n",
      "epoch 145, minibatch 500/500, test error 20.150000 %\n",
      "training @ iter =  72500\n",
      "0.656010270119\n",
      "training @ iter =  72600\n",
      "0.610734581947\n",
      "training @ iter =  72700\n",
      "0.569425821304\n",
      "training @ iter =  72800\n",
      "0.888285815716\n",
      "training @ iter =  72900\n",
      "0.561297655106\n",
      "epoch 146, minibatch 500/500, test error 18.600000 %\n",
      "training @ iter =  73000\n",
      "0.621049284935\n",
      "training @ iter =  73100\n",
      "0.628541588783\n",
      "training @ iter =  73200\n",
      "0.649252414703\n",
      "training @ iter =  73300\n",
      "0.678336560726\n",
      "training @ iter =  73400\n",
      "0.620771884918\n",
      "epoch 147, minibatch 500/500, test error 19.090000 %\n",
      "training @ iter =  73500\n",
      "0.706386327744\n",
      "training @ iter =  73600\n",
      "0.608985841274\n",
      "training @ iter =  73700\n",
      "0.498736560345\n",
      "training @ iter =  73800\n",
      "0.626856327057\n",
      "training @ iter =  73900\n",
      "0.43110075593\n",
      "epoch 148, minibatch 500/500, test error 19.750000 %\n",
      "training @ iter =  74000\n",
      "0.582048654556\n",
      "training @ iter =  74100\n",
      "0.448849916458\n",
      "training @ iter =  74200\n",
      "0.674651801586\n",
      "training @ iter =  74300\n",
      "0.631653785706\n",
      "training @ iter =  74400\n",
      "0.478103607893\n",
      "epoch 149, minibatch 500/500, test error 20.200000 %\n",
      "training @ iter =  74500\n",
      "0.598011612892\n",
      "training @ iter =  74600\n",
      "0.561465203762\n",
      "training @ iter =  74700\n",
      "0.48435652256\n",
      "training @ iter =  74800\n",
      "0.630297839642\n",
      "training @ iter =  74900\n",
      "0.62668555975\n",
      "epoch 150, minibatch 500/500, test error 20.640000 %\n",
      "training @ iter =  75000\n",
      "0.657778143883\n",
      "training @ iter =  75100\n",
      "0.567243635654\n",
      "training @ iter =  75200\n",
      "0.642688155174\n",
      "training @ iter =  75300\n",
      "0.78870421648\n",
      "training @ iter =  75400\n",
      "0.625332593918\n",
      "epoch 151, minibatch 500/500, test error 20.720000 %\n",
      "training @ iter =  75500\n",
      "0.675363361835\n",
      "training @ iter =  75600\n",
      "0.54704874754\n",
      "training @ iter =  75700\n",
      "0.567919969559\n",
      "training @ iter =  75800\n",
      "0.693947017193\n",
      "training @ iter =  75900\n",
      "0.621014356613\n",
      "epoch 152, minibatch 500/500, test error 19.830000 %\n",
      "training @ iter =  76000\n",
      "0.611050903797\n",
      "training @ iter =  76100\n",
      "0.512261152267\n",
      "training @ iter =  76200\n",
      "0.590060114861\n",
      "training @ iter =  76300\n",
      "0.673225581646\n",
      "training @ iter =  76400\n",
      "0.62723004818\n",
      "epoch 153, minibatch 500/500, test error 19.350000 %\n",
      "training @ iter =  76500\n",
      "0.614709913731\n",
      "training @ iter =  76600\n",
      "0.618753194809\n",
      "training @ iter =  76700\n",
      "0.611642062664\n",
      "training @ iter =  76800\n",
      "0.727006733418\n",
      "training @ iter =  76900\n",
      "0.553255796432\n",
      "epoch 154, minibatch 500/500, test error 19.270000 %\n",
      "training @ iter =  77000\n",
      "0.73715955019\n",
      "training @ iter =  77100\n",
      "0.601336300373\n",
      "training @ iter =  77200\n",
      "0.641452789307\n",
      "training @ iter =  77300\n",
      "0.635084211826\n",
      "training @ iter =  77400\n",
      "0.576429367065\n",
      "epoch 155, minibatch 500/500, test error 19.200000 %\n",
      "training @ iter =  77500\n",
      "0.747020542622\n",
      "training @ iter =  77600\n",
      "0.673589110374\n",
      "training @ iter =  77700\n",
      "0.638552308083\n",
      "training @ iter =  77800\n",
      "0.753763139248\n",
      "training @ iter =  77900\n",
      "0.67993158102\n",
      "epoch 156, minibatch 500/500, test error 19.160000 %\n",
      "training @ iter =  78000\n",
      "0.616771876812\n",
      "training @ iter =  78100\n",
      "0.458245664835\n",
      "training @ iter =  78200\n",
      "0.716922461987\n",
      "training @ iter =  78300\n",
      "0.564792513847\n",
      "training @ iter =  78400\n",
      "0.627317547798\n",
      "epoch 157, minibatch 500/500, test error 19.860000 %\n",
      "training @ iter =  78500\n",
      "0.741243302822\n",
      "training @ iter =  78600\n",
      "0.656132876873\n",
      "training @ iter =  78700\n",
      "0.657281517982\n",
      "training @ iter =  78800\n",
      "0.575702250004\n",
      "training @ iter =  78900\n",
      "0.516398668289\n",
      "epoch 158, minibatch 500/500, test error 20.050000 %\n",
      "training @ iter =  79000\n",
      "0.750038921833\n",
      "training @ iter =  79100\n",
      "0.723214507103\n",
      "training @ iter =  79200\n",
      "0.716739177704\n",
      "training @ iter =  79300\n",
      "0.686257481575\n",
      "training @ iter =  79400\n",
      "0.582784473896\n",
      "epoch 159, minibatch 500/500, test error 20.520000 %\n",
      "training @ iter =  79500\n",
      "0.648493051529\n",
      "training @ iter =  79600\n",
      "0.559213340282\n",
      "training @ iter =  79700\n",
      "0.619364798069\n",
      "training @ iter =  79800\n",
      "0.732161104679\n",
      "training @ iter =  79900\n",
      "0.42470023036\n",
      "epoch 160, minibatch 500/500, test error 19.380000 %\n",
      "training @ iter =  80000\n",
      "0.553878486156\n",
      "training @ iter =  80100\n",
      "0.540367484093\n",
      "training @ iter =  80200\n",
      "0.593170166016\n",
      "training @ iter =  80300\n",
      "0.61344397068\n",
      "training @ iter =  80400\n",
      "0.49986821413\n",
      "epoch 161, minibatch 500/500, test error 20.400000 %\n",
      "training @ iter =  80500\n",
      "0.607044696808\n",
      "training @ iter =  80600\n",
      "0.704439997673\n",
      "training @ iter =  80700\n",
      "0.590814948082\n",
      "training @ iter =  80800\n",
      "0.657602846622\n",
      "training @ iter =  80900\n",
      "0.537826657295\n",
      "epoch 162, minibatch 500/500, test error 19.640000 %\n",
      "training @ iter =  81000\n",
      "0.655748128891\n",
      "training @ iter =  81100\n",
      "0.616018772125\n",
      "training @ iter =  81200\n",
      "0.697343289852\n",
      "training @ iter =  81300\n",
      "0.935608625412\n",
      "training @ iter =  81400\n",
      "0.628224611282\n",
      "epoch 163, minibatch 500/500, test error 19.840000 %\n",
      "training @ iter =  81500\n",
      "0.522510051727\n",
      "training @ iter =  81600\n",
      "0.486020505428\n",
      "training @ iter =  81700\n",
      "0.472306668758\n",
      "training @ iter =  81800\n",
      "0.569482505322\n",
      "training @ iter =  81900\n",
      "0.546009540558\n",
      "epoch 164, minibatch 500/500, test error 20.360000 %\n",
      "training @ iter =  82000\n",
      "0.668073296547\n",
      "training @ iter =  82100\n",
      "0.571912825108\n",
      "training @ iter =  82200\n",
      "0.625527739525\n",
      "training @ iter =  82300\n",
      "0.661005079746\n",
      "training @ iter =  82400\n",
      "0.678958654404\n",
      "epoch 165, minibatch 500/500, test error 19.550000 %\n",
      "training @ iter =  82500\n",
      "0.669873356819\n",
      "training @ iter =  82600\n",
      "0.666101813316\n",
      "training @ iter =  82700\n",
      "0.661056041718\n",
      "training @ iter =  82800\n",
      "0.690375089645\n",
      "training @ iter =  82900\n",
      "0.742327749729\n",
      "epoch 166, minibatch 500/500, test error 19.810000 %\n",
      "training @ iter =  83000\n",
      "0.684569954872\n",
      "training @ iter =  83100\n",
      "0.664552628994\n",
      "training @ iter =  83200\n",
      "0.862001478672\n",
      "training @ iter =  83300\n",
      "0.690563678741\n",
      "training @ iter =  83400\n",
      "0.770050287247\n",
      "epoch 167, minibatch 500/500, test error 21.810000 %\n",
      "training @ iter =  83500\n",
      "0.769697248936\n",
      "training @ iter =  83600\n",
      "0.731269598007\n",
      "training @ iter =  83700\n",
      "0.658865988255\n",
      "training @ iter =  83800\n",
      "0.704190850258\n",
      "training @ iter =  83900\n",
      "0.555007398129\n",
      "epoch 168, minibatch 500/500, test error 19.780000 %\n",
      "training @ iter =  84000\n",
      "0.679042637348\n",
      "training @ iter =  84100\n",
      "0.68262720108\n",
      "training @ iter =  84200\n",
      "0.624608457088\n",
      "training @ iter =  84300\n",
      "0.708033621311\n",
      "training @ iter =  84400\n",
      "0.606496214867\n",
      "epoch 169, minibatch 500/500, test error 20.010000 %\n",
      "training @ iter =  84500\n",
      "0.604152083397\n",
      "training @ iter =  84600\n",
      "0.591782927513\n",
      "training @ iter =  84700\n",
      "0.636232256889\n",
      "training @ iter =  84800\n",
      "0.762248635292\n",
      "training @ iter =  84900\n",
      "0.77438634634\n",
      "epoch 170, minibatch 500/500, test error 20.060000 %\n",
      "training @ iter =  85000\n",
      "0.620314776897\n",
      "training @ iter =  85100\n",
      "0.671118497849\n",
      "training @ iter =  85200\n",
      "0.695631325245\n",
      "training @ iter =  85300\n",
      "0.561646103859\n",
      "training @ iter =  85400\n",
      "0.661133408546\n",
      "epoch 171, minibatch 500/500, test error 21.020000 %\n",
      "training @ iter =  85500\n",
      "0.637555837631\n",
      "training @ iter =  85600\n",
      "0.505395412445\n",
      "training @ iter =  85700\n",
      "0.658653616905\n",
      "training @ iter =  85800\n",
      "0.752155601978\n",
      "training @ iter =  85900\n",
      "0.5972661376\n",
      "epoch 172, minibatch 500/500, test error 20.210000 %\n",
      "training @ iter =  86000\n",
      "0.635099112988\n",
      "training @ iter =  86100\n",
      "0.587814331055\n",
      "training @ iter =  86200\n",
      "0.659880816936\n",
      "training @ iter =  86300\n",
      "0.597191572189\n",
      "training @ iter =  86400\n",
      "0.646459281445\n",
      "epoch 173, minibatch 500/500, test error 19.890000 %\n",
      "training @ iter =  86500\n",
      "0.529350280762\n",
      "training @ iter =  86600\n",
      "0.521571516991\n",
      "training @ iter =  86700\n",
      "0.646658301353\n",
      "training @ iter =  86800\n",
      "0.698804795742\n",
      "training @ iter =  86900\n",
      "0.525268852711\n",
      "epoch 174, minibatch 500/500, test error 19.630000 %\n",
      "training @ iter =  87000\n",
      "0.76342099905\n",
      "training @ iter =  87100\n",
      "0.721919119358\n",
      "training @ iter =  87200\n",
      "0.785752177238\n",
      "training @ iter =  87300\n",
      "0.666812717915\n",
      "training @ iter =  87400\n",
      "0.647445499897\n",
      "epoch 175, minibatch 500/500, test error 20.660000 %\n",
      "training @ iter =  87500\n",
      "0.665823757648\n",
      "training @ iter =  87600\n",
      "0.505073606968\n",
      "training @ iter =  87700\n",
      "0.647405147552\n",
      "training @ iter =  87800\n",
      "0.633232235909\n",
      "training @ iter =  87900\n",
      "0.663111746311\n",
      "epoch 176, minibatch 500/500, test error 20.160000 %\n",
      "training @ iter =  88000\n",
      "0.642967760563\n",
      "training @ iter =  88100\n",
      "0.606705963612\n",
      "training @ iter =  88200\n",
      "0.611222982407\n",
      "training @ iter =  88300\n",
      "0.76388669014\n",
      "training @ iter =  88400\n",
      "0.450912177563\n",
      "epoch 177, minibatch 500/500, test error 20.080000 %\n",
      "training @ iter =  88500\n",
      "0.635393917561\n",
      "training @ iter =  88600\n",
      "0.611618101597\n",
      "training @ iter =  88700\n",
      "0.644556760788\n",
      "training @ iter =  88800\n",
      "0.717148661613\n",
      "training @ iter =  88900\n",
      "0.732050418854\n",
      "epoch 178, minibatch 500/500, test error 20.040000 %\n",
      "training @ iter =  89000\n",
      "0.683076024055\n",
      "training @ iter =  89100\n",
      "0.691596806049\n",
      "training @ iter =  89200\n",
      "0.659392237663\n",
      "training @ iter =  89300\n",
      "0.530289709568\n",
      "training @ iter =  89400\n",
      "0.603969633579\n",
      "epoch 179, minibatch 500/500, test error 19.330000 %\n",
      "training @ iter =  89500\n",
      "0.629144847393\n",
      "training @ iter =  89600\n",
      "0.515612959862\n",
      "training @ iter =  89700\n",
      "0.557734727859\n",
      "training @ iter =  89800\n",
      "0.678702771664\n",
      "training @ iter =  89900\n",
      "0.435884475708\n",
      "epoch 180, minibatch 500/500, test error 20.510000 %\n",
      "training @ iter =  90000\n",
      "0.752176821232\n",
      "training @ iter =  90100\n",
      "0.605945289135\n",
      "training @ iter =  90200\n",
      "0.85024189949\n",
      "training @ iter =  90300\n",
      "0.645766854286\n",
      "training @ iter =  90400\n",
      "0.504148244858\n",
      "epoch 181, minibatch 500/500, test error 19.740000 %\n",
      "training @ iter =  90500\n",
      "0.783176600933\n",
      "training @ iter =  90600\n",
      "0.618441402912\n",
      "training @ iter =  90700\n",
      "0.628963410854\n",
      "training @ iter =  90800\n",
      "0.720450282097\n",
      "training @ iter =  90900\n",
      "0.479933768511\n",
      "epoch 182, minibatch 500/500, test error 20.000000 %\n",
      "training @ iter =  91000\n",
      "0.751992881298\n",
      "training @ iter =  91100\n",
      "0.477983444929\n",
      "training @ iter =  91200\n",
      "0.710750937462\n",
      "training @ iter =  91300\n",
      "0.683476209641\n",
      "training @ iter =  91400\n",
      "0.786170482635\n",
      "epoch 183, minibatch 500/500, test error 21.920000 %\n",
      "training @ iter =  91500\n",
      "0.636473476887\n",
      "training @ iter =  91600\n",
      "0.642889082432\n",
      "training @ iter =  91700\n",
      "0.523604571819\n",
      "training @ iter =  91800\n",
      "0.879616379738\n",
      "training @ iter =  91900\n",
      "0.597975969315\n",
      "epoch 184, minibatch 500/500, test error 21.120000 %\n",
      "training @ iter =  92000\n",
      "0.682436227798\n",
      "training @ iter =  92100\n",
      "0.664569497108\n",
      "training @ iter =  92200\n",
      "0.75448089838\n",
      "training @ iter =  92300\n",
      "0.667306303978\n",
      "training @ iter =  92400\n",
      "0.646992743015\n",
      "epoch 185, minibatch 500/500, test error 19.200000 %\n",
      "training @ iter =  92500\n",
      "0.66613817215\n",
      "training @ iter =  92600\n",
      "0.530472576618\n",
      "training @ iter =  92700\n",
      "0.656342327595\n",
      "training @ iter =  92800\n",
      "0.678118526936\n",
      "training @ iter =  92900\n",
      "0.659492254257\n",
      "epoch 186, minibatch 500/500, test error 19.960000 %\n",
      "training @ iter =  93000\n",
      "0.583533704281\n",
      "training @ iter =  93100\n",
      "0.546043753624\n",
      "training @ iter =  93200\n",
      "0.594767868519\n",
      "training @ iter =  93300\n",
      "0.576881229877\n",
      "training @ iter =  93400\n",
      "0.465301930904\n",
      "epoch 187, minibatch 500/500, test error 19.500000 %\n",
      "training @ iter =  93500\n",
      "0.635200738907\n",
      "training @ iter =  93600\n",
      "0.624882102013\n",
      "training @ iter =  93700\n",
      "0.658410012722\n",
      "training @ iter =  93800\n",
      "0.628741502762\n",
      "training @ iter =  93900\n",
      "0.598789572716\n",
      "epoch 188, minibatch 500/500, test error 20.060000 %\n",
      "training @ iter =  94000\n",
      "0.740961551666\n",
      "training @ iter =  94100\n",
      "0.628088712692\n",
      "training @ iter =  94200\n",
      "0.750935614109\n",
      "training @ iter =  94300\n",
      "0.691478490829\n",
      "training @ iter =  94400\n",
      "0.568868875504\n",
      "epoch 189, minibatch 500/500, test error 20.230000 %\n",
      "training @ iter =  94500\n",
      "0.571106314659\n",
      "training @ iter =  94600\n",
      "0.630069732666\n",
      "training @ iter =  94700\n",
      "0.789565861225\n",
      "training @ iter =  94800\n",
      "0.614297807217\n",
      "training @ iter =  94900\n",
      "0.57422965765\n",
      "epoch 190, minibatch 500/500, test error 20.470000 %\n",
      "training @ iter =  95000\n",
      "0.612472891808\n",
      "training @ iter =  95100\n",
      "0.586314022541\n",
      "training @ iter =  95200\n",
      "0.781836092472\n",
      "training @ iter =  95300\n",
      "0.554702281952\n",
      "training @ iter =  95400\n",
      "0.617391943932\n",
      "epoch 191, minibatch 500/500, test error 20.640000 %\n",
      "training @ iter =  95500\n",
      "0.694724202156\n",
      "training @ iter =  95600\n",
      "0.658328473568\n",
      "training @ iter =  95700\n",
      "0.675479888916\n",
      "training @ iter =  95800\n",
      "0.620818436146\n",
      "training @ iter =  95900\n",
      "0.673283815384\n",
      "epoch 192, minibatch 500/500, test error 20.820000 %\n",
      "training @ iter =  96000\n",
      "0.669080674648\n",
      "training @ iter =  96100\n",
      "0.612976253033\n",
      "training @ iter =  96200\n",
      "0.655325710773\n",
      "training @ iter =  96300\n",
      "0.590135097504\n",
      "training @ iter =  96400\n",
      "0.550396203995\n",
      "epoch 193, minibatch 500/500, test error 19.960000 %\n",
      "training @ iter =  96500\n",
      "0.645242333412\n",
      "training @ iter =  96600\n",
      "0.698516070843\n",
      "training @ iter =  96700\n",
      "0.655045747757\n",
      "training @ iter =  96800\n",
      "0.701134860516\n",
      "training @ iter =  96900\n",
      "0.594080030918\n",
      "epoch 194, minibatch 500/500, test error 20.260000 %\n",
      "training @ iter =  97000\n",
      "0.670857608318\n",
      "training @ iter =  97100\n",
      "0.621495187283\n",
      "training @ iter =  97200\n",
      "0.640092253685\n",
      "training @ iter =  97300\n",
      "0.566871404648\n",
      "training @ iter =  97400\n",
      "0.696460664272\n",
      "epoch 195, minibatch 500/500, test error 20.570000 %\n",
      "training @ iter =  97500\n",
      "0.528925180435\n",
      "training @ iter =  97600\n",
      "0.636632859707\n",
      "training @ iter =  97700\n",
      "0.846329808235\n",
      "training @ iter =  97800\n",
      "0.742951571941\n",
      "training @ iter =  97900\n",
      "0.525321125984\n",
      "epoch 196, minibatch 500/500, test error 21.270000 %\n",
      "training @ iter =  98000\n",
      "0.607224345207\n",
      "training @ iter =  98100\n",
      "0.754414081573\n",
      "training @ iter =  98200\n",
      "0.589161276817\n",
      "training @ iter =  98300\n",
      "0.782882511616\n",
      "training @ iter =  98400\n",
      "0.691136956215\n",
      "epoch 197, minibatch 500/500, test error 21.320000 %\n",
      "training @ iter =  98500\n",
      "0.641544520855\n",
      "training @ iter =  98600\n",
      "0.544973492622\n",
      "training @ iter =  98700\n",
      "0.725956499577\n",
      "training @ iter =  98800\n",
      "0.705424785614\n",
      "training @ iter =  98900\n",
      "0.485011905432\n",
      "epoch 198, minibatch 500/500, test error 20.120000 %\n",
      "training @ iter =  99000\n",
      "0.622624218464\n",
      "training @ iter =  99100\n",
      "0.693807780743\n",
      "training @ iter =  99200\n",
      "0.649931252003\n",
      "training @ iter =  99300\n",
      "0.72663885355\n",
      "training @ iter =  99400\n",
      "0.699800550938\n",
      "epoch 199, minibatch 500/500, test error 20.170000 %\n",
      "training @ iter =  99500\n",
      "0.679383218288\n",
      "training @ iter =  99600\n",
      "0.781437754631\n",
      "training @ iter =  99700\n",
      "0.505137503147\n",
      "training @ iter =  99800\n",
      "0.70493209362\n",
      "training @ iter =  99900\n",
      "0.574474811554\n",
      "epoch 200, minibatch 500/500, test error 19.830000 %\n",
      "training @ iter =  100000\n",
      "0.712029874325\n",
      "training @ iter =  100100\n",
      "0.609304368496\n",
      "training @ iter =  100200\n",
      "0.597265303135\n",
      "training @ iter =  100300\n",
      "0.570550620556\n",
      "training @ iter =  100400\n",
      "0.328088909388\n",
      "epoch 201, minibatch 500/500, test error 18.050000 %\n",
      "training @ iter =  100500\n",
      "0.507269382477\n",
      "training @ iter =  100600\n",
      "0.473268538713\n",
      "training @ iter =  100700\n",
      "0.584356129169\n",
      "training @ iter =  100800\n",
      "0.546523272991\n",
      "training @ iter =  100900\n",
      "0.508868277073\n",
      "epoch 202, minibatch 500/500, test error 17.730000 %\n",
      "training @ iter =  101000\n",
      "0.627638220787\n",
      "training @ iter =  101100\n",
      "0.445866346359\n",
      "training @ iter =  101200\n",
      "0.505843281746\n",
      "training @ iter =  101300\n",
      "0.522407591343\n",
      "training @ iter =  101400\n",
      "0.464099109173\n",
      "epoch 203, minibatch 500/500, test error 17.360000 %\n",
      "training @ iter =  101500\n",
      "0.68964111805\n",
      "training @ iter =  101600\n",
      "0.424350142479\n",
      "training @ iter =  101700\n",
      "0.409950852394\n",
      "training @ iter =  101800\n",
      "0.5586348176\n",
      "training @ iter =  101900\n",
      "0.418563991785\n",
      "epoch 204, minibatch 500/500, test error 17.230000 %\n",
      "training @ iter =  102000\n",
      "0.60122948885\n",
      "training @ iter =  102100\n",
      "0.460114121437\n",
      "training @ iter =  102200\n",
      "0.500160872936\n",
      "training @ iter =  102300\n",
      "0.486263155937\n",
      "training @ iter =  102400\n",
      "0.424043804407\n",
      "epoch 205, minibatch 500/500, test error 17.330000 %\n",
      "training @ iter =  102500\n",
      "0.492310106754\n",
      "training @ iter =  102600\n",
      "0.420912384987\n",
      "training @ iter =  102700\n",
      "0.466813594103\n",
      "training @ iter =  102800\n",
      "0.577363193035\n",
      "training @ iter =  102900\n",
      "0.333939939737\n",
      "epoch 206, minibatch 500/500, test error 16.910000 %\n",
      "training @ iter =  103000\n",
      "0.522006630898\n",
      "training @ iter =  103100\n",
      "0.379406839609\n",
      "training @ iter =  103200\n",
      "0.443442225456\n",
      "training @ iter =  103300\n",
      "0.539788722992\n",
      "training @ iter =  103400\n",
      "0.365756452084\n",
      "epoch 207, minibatch 500/500, test error 16.840000 %\n",
      "training @ iter =  103500\n",
      "0.558993637562\n",
      "training @ iter =  103600\n",
      "0.429094016552\n",
      "training @ iter =  103700\n",
      "0.517065644264\n",
      "training @ iter =  103800\n",
      "0.46612367034\n",
      "training @ iter =  103900\n",
      "0.532011032104\n",
      "epoch 208, minibatch 500/500, test error 16.890000 %\n",
      "training @ iter =  104000\n",
      "0.569704413414\n",
      "training @ iter =  104100\n",
      "0.428055197001\n",
      "training @ iter =  104200\n",
      "0.595147371292\n",
      "training @ iter =  104300\n",
      "0.497190117836\n",
      "training @ iter =  104400\n",
      "0.371511012316\n",
      "epoch 209, minibatch 500/500, test error 16.740000 %\n",
      "training @ iter =  104500\n",
      "0.393647700548\n",
      "training @ iter =  104600\n",
      "0.468094825745\n",
      "training @ iter =  104700\n",
      "0.494637221098\n",
      "training @ iter =  104800\n",
      "0.518401265144\n",
      "training @ iter =  104900\n",
      "0.389740824699\n",
      "epoch 210, minibatch 500/500, test error 16.770000 %\n",
      "training @ iter =  105000\n",
      "0.56521832943\n",
      "training @ iter =  105100\n",
      "0.41585880518\n",
      "training @ iter =  105200\n",
      "0.51176071167\n",
      "training @ iter =  105300\n",
      "0.530484318733\n",
      "training @ iter =  105400\n",
      "0.411025583744\n",
      "epoch 211, minibatch 500/500, test error 16.580000 %\n",
      "training @ iter =  105500\n",
      "0.605789661407\n",
      "training @ iter =  105600\n",
      "0.488105326891\n",
      "training @ iter =  105700\n",
      "0.531823039055\n",
      "training @ iter =  105800\n",
      "0.512338340282\n",
      "training @ iter =  105900\n",
      "0.381516188383\n",
      "epoch 212, minibatch 500/500, test error 16.870000 %\n",
      "training @ iter =  106000\n",
      "0.381890445948\n",
      "training @ iter =  106100\n",
      "0.34002199769\n",
      "training @ iter =  106200\n",
      "0.509676754475\n",
      "training @ iter =  106300\n",
      "0.439594388008\n",
      "training @ iter =  106400\n",
      "0.402243435383\n",
      "epoch 213, minibatch 500/500, test error 16.510000 %\n",
      "training @ iter =  106500\n",
      "0.363165974617\n",
      "training @ iter =  106600\n",
      "0.334078103304\n",
      "training @ iter =  106700\n",
      "0.458791851997\n",
      "training @ iter =  106800\n",
      "0.558845341206\n",
      "training @ iter =  106900\n",
      "0.399974107742\n",
      "epoch 214, minibatch 500/500, test error 16.340000 %\n",
      "training @ iter =  107000\n",
      "0.469738513231\n",
      "training @ iter =  107100\n",
      "0.404784023762\n",
      "training @ iter =  107200\n",
      "0.441461265087\n",
      "training @ iter =  107300\n",
      "0.537966489792\n",
      "training @ iter =  107400\n",
      "0.372424900532\n",
      "epoch 215, minibatch 500/500, test error 16.330000 %\n",
      "training @ iter =  107500\n",
      "0.310406446457\n",
      "training @ iter =  107600\n",
      "0.411592394114\n",
      "training @ iter =  107700\n",
      "0.370060801506\n",
      "training @ iter =  107800\n",
      "0.579848647118\n",
      "training @ iter =  107900\n",
      "0.414734184742\n",
      "epoch 216, minibatch 500/500, test error 16.280000 %\n",
      "training @ iter =  108000\n",
      "0.576942980289\n",
      "training @ iter =  108100\n",
      "0.389911234379\n",
      "training @ iter =  108200\n",
      "0.364229351282\n",
      "training @ iter =  108300\n",
      "0.40937846899\n",
      "training @ iter =  108400\n",
      "0.372807323933\n",
      "epoch 217, minibatch 500/500, test error 16.540000 %\n",
      "training @ iter =  108500\n",
      "0.425089329481\n",
      "training @ iter =  108600\n",
      "0.366484105587\n",
      "training @ iter =  108700\n",
      "0.364489644766\n",
      "training @ iter =  108800\n",
      "0.583600282669\n",
      "training @ iter =  108900\n",
      "0.411257714033\n",
      "epoch 218, minibatch 500/500, test error 16.460000 %\n",
      "training @ iter =  109000\n",
      "0.33868765831\n",
      "training @ iter =  109100\n",
      "0.423163056374\n",
      "training @ iter =  109200\n",
      "0.520317256451\n",
      "training @ iter =  109300\n",
      "0.434700548649\n",
      "training @ iter =  109400\n",
      "0.396021544933\n",
      "epoch 219, minibatch 500/500, test error 16.340000 %\n",
      "training @ iter =  109500\n",
      "0.444841116667\n",
      "training @ iter =  109600\n",
      "0.408480525017\n",
      "training @ iter =  109700\n",
      "0.342890232801\n",
      "training @ iter =  109800\n",
      "0.476799279451\n",
      "training @ iter =  109900\n",
      "0.31255543232\n",
      "epoch 220, minibatch 500/500, test error 16.220000 %\n",
      "training @ iter =  110000\n",
      "0.533837199211\n",
      "training @ iter =  110100\n",
      "0.361967176199\n",
      "training @ iter =  110200\n",
      "0.642967700958\n",
      "training @ iter =  110300\n",
      "0.506671130657\n",
      "training @ iter =  110400\n",
      "0.383616805077\n",
      "epoch 221, minibatch 500/500, test error 16.440000 %\n",
      "training @ iter =  110500\n",
      "0.520438611507\n",
      "training @ iter =  110600\n",
      "0.421111106873\n",
      "training @ iter =  110700\n",
      "0.360461622477\n",
      "training @ iter =  110800\n",
      "0.471105694771\n",
      "training @ iter =  110900\n",
      "0.402284622192\n",
      "epoch 222, minibatch 500/500, test error 16.070000 %\n",
      "training @ iter =  111000\n",
      "0.399008721113\n",
      "training @ iter =  111100\n",
      "0.425868958235\n",
      "training @ iter =  111200\n",
      "0.466507822275\n",
      "training @ iter =  111300\n",
      "0.497815012932\n",
      "training @ iter =  111400\n",
      "0.375597447157\n",
      "epoch 223, minibatch 500/500, test error 16.360000 %\n",
      "training @ iter =  111500\n",
      "0.406843185425\n",
      "training @ iter =  111600\n",
      "0.363408207893\n",
      "training @ iter =  111700\n",
      "0.45097053051\n",
      "training @ iter =  111800\n",
      "0.498521506786\n",
      "training @ iter =  111900\n",
      "0.329891353846\n",
      "epoch 224, minibatch 500/500, test error 16.040000 %\n",
      "training @ iter =  112000\n",
      "0.429843991995\n",
      "training @ iter =  112100\n",
      "0.289891391993\n",
      "training @ iter =  112200\n",
      "0.437471956015\n",
      "training @ iter =  112300\n",
      "0.521619796753\n",
      "training @ iter =  112400\n",
      "0.399877041578\n",
      "epoch 225, minibatch 500/500, test error 15.830000 %\n",
      "training @ iter =  112500\n",
      "0.471175014973\n",
      "training @ iter =  112600\n",
      "0.433501243591\n",
      "training @ iter =  112700\n",
      "0.427034467459\n",
      "training @ iter =  112800\n",
      "0.39277780056\n",
      "training @ iter =  112900\n",
      "0.37806481123\n",
      "epoch 226, minibatch 500/500, test error 16.200000 %\n",
      "training @ iter =  113000\n",
      "0.470707535744\n",
      "training @ iter =  113100\n",
      "0.41606643796\n",
      "training @ iter =  113200\n",
      "0.445319950581\n",
      "training @ iter =  113300\n",
      "0.452097326517\n",
      "training @ iter =  113400\n",
      "0.402997195721\n",
      "epoch 227, minibatch 500/500, test error 16.050000 %\n",
      "training @ iter =  113500\n",
      "0.440390348434\n",
      "training @ iter =  113600\n",
      "0.40936934948\n",
      "training @ iter =  113700\n",
      "0.384567797184\n",
      "training @ iter =  113800\n",
      "0.419700235128\n",
      "training @ iter =  113900\n",
      "0.354210674763\n",
      "epoch 228, minibatch 500/500, test error 16.320000 %\n",
      "training @ iter =  114000\n",
      "0.484258204699\n",
      "training @ iter =  114100\n",
      "0.461686044931\n",
      "training @ iter =  114200\n",
      "0.362941980362\n",
      "training @ iter =  114300\n",
      "0.529048740864\n",
      "training @ iter =  114400\n",
      "0.326150536537\n",
      "epoch 229, minibatch 500/500, test error 16.320000 %\n",
      "training @ iter =  114500\n",
      "0.430833429098\n",
      "training @ iter =  114600\n",
      "0.413765609264\n",
      "training @ iter =  114700\n",
      "0.41859254241\n",
      "training @ iter =  114800\n",
      "0.43208488822\n",
      "training @ iter =  114900\n",
      "0.335622698069\n",
      "epoch 230, minibatch 500/500, test error 15.930000 %\n",
      "training @ iter =  115000\n",
      "0.501773476601\n",
      "training @ iter =  115100\n",
      "0.430322915316\n",
      "training @ iter =  115200\n",
      "0.480670541525\n",
      "training @ iter =  115300\n",
      "0.479285359383\n",
      "training @ iter =  115400\n",
      "0.389194846153\n",
      "epoch 231, minibatch 500/500, test error 15.810000 %\n",
      "training @ iter =  115500\n",
      "0.457426846027\n",
      "training @ iter =  115600\n",
      "0.428612351418\n",
      "training @ iter =  115700\n",
      "0.420051962137\n",
      "training @ iter =  115800\n",
      "0.420495212078\n",
      "training @ iter =  115900\n",
      "0.389007985592\n",
      "epoch 232, minibatch 500/500, test error 15.770000 %\n",
      "training @ iter =  116000\n",
      "0.442994773388\n",
      "training @ iter =  116100\n",
      "0.318082094193\n",
      "training @ iter =  116200\n",
      "0.466617584229\n",
      "training @ iter =  116300\n",
      "0.591224491596\n",
      "training @ iter =  116400\n",
      "0.382688224316\n",
      "epoch 233, minibatch 500/500, test error 16.010000 %\n",
      "training @ iter =  116500\n",
      "0.440306961536\n",
      "training @ iter =  116600\n",
      "0.296333402395\n",
      "training @ iter =  116700\n",
      "0.349251300097\n",
      "training @ iter =  116800\n",
      "0.36463829875\n",
      "training @ iter =  116900\n",
      "0.348971009254\n",
      "epoch 234, minibatch 500/500, test error 15.910000 %\n",
      "training @ iter =  117000\n",
      "0.460745960474\n",
      "training @ iter =  117100\n",
      "0.501417756081\n",
      "training @ iter =  117200\n",
      "0.337003558874\n",
      "training @ iter =  117300\n",
      "0.470124512911\n",
      "training @ iter =  117400\n",
      "0.358285069466\n",
      "epoch 235, minibatch 500/500, test error 15.790000 %\n",
      "training @ iter =  117500\n",
      "0.426103293896\n",
      "training @ iter =  117600\n",
      "0.460989922285\n",
      "training @ iter =  117700\n",
      "0.464803397655\n",
      "training @ iter =  117800\n",
      "0.483905225992\n",
      "training @ iter =  117900\n",
      "0.281882256269\n",
      "epoch 236, minibatch 500/500, test error 15.710000 %\n",
      "training @ iter =  118000\n",
      "0.411909908056\n",
      "training @ iter =  118100\n",
      "0.431744992733\n",
      "training @ iter =  118200\n",
      "0.449076294899\n",
      "training @ iter =  118300\n",
      "0.446988463402\n",
      "training @ iter =  118400\n",
      "0.405927479267\n",
      "epoch 237, minibatch 500/500, test error 15.810000 %\n",
      "training @ iter =  118500\n",
      "0.527820885181\n",
      "training @ iter =  118600\n",
      "0.339420706034\n",
      "training @ iter =  118700\n",
      "0.425821721554\n",
      "training @ iter =  118800\n",
      "0.520090639591\n",
      "training @ iter =  118900\n",
      "0.350102245808\n",
      "epoch 238, minibatch 500/500, test error 15.790000 %\n",
      "training @ iter =  119000\n",
      "0.518866717815\n",
      "training @ iter =  119100\n",
      "0.385838240385\n",
      "training @ iter =  119200\n",
      "0.503954291344\n",
      "training @ iter =  119300\n",
      "0.360659956932\n",
      "training @ iter =  119400\n",
      "0.422949671745\n",
      "epoch 239, minibatch 500/500, test error 15.660000 %\n",
      "training @ iter =  119500\n",
      "0.415326088667\n",
      "training @ iter =  119600\n",
      "0.349555432796\n",
      "training @ iter =  119700\n",
      "0.346163332462\n",
      "training @ iter =  119800\n",
      "0.568453073502\n",
      "training @ iter =  119900\n",
      "0.374806523323\n",
      "epoch 240, minibatch 500/500, test error 15.800000 %\n",
      "training @ iter =  120000\n",
      "0.375180155039\n",
      "training @ iter =  120100\n",
      "0.310423433781\n",
      "training @ iter =  120200\n",
      "0.43228751421\n",
      "training @ iter =  120300\n",
      "0.620157003403\n",
      "training @ iter =  120400\n",
      "0.327547729015\n",
      "epoch 241, minibatch 500/500, test error 15.900000 %\n",
      "training @ iter =  120500\n",
      "0.513136804104\n",
      "training @ iter =  120600\n",
      "0.330137938261\n",
      "training @ iter =  120700\n",
      "0.403730481863\n",
      "training @ iter =  120800\n",
      "0.414661169052\n",
      "training @ iter =  120900\n",
      "0.336946099997\n",
      "epoch 242, minibatch 500/500, test error 15.590000 %\n",
      "training @ iter =  121000\n",
      "0.377436339855\n",
      "training @ iter =  121100\n",
      "0.368804365396\n",
      "training @ iter =  121200\n",
      "0.325064480305\n",
      "training @ iter =  121300\n",
      "0.52392333746\n",
      "training @ iter =  121400\n",
      "0.438782036304\n",
      "epoch 243, minibatch 500/500, test error 15.630000 %\n",
      "training @ iter =  121500\n",
      "0.406030267477\n",
      "training @ iter =  121600\n",
      "0.429293930531\n",
      "training @ iter =  121700\n",
      "0.432209312916\n",
      "training @ iter =  121800\n",
      "0.523714661598\n",
      "training @ iter =  121900\n",
      "0.398229718208\n",
      "epoch 244, minibatch 500/500, test error 15.420000 %\n",
      "training @ iter =  122000\n",
      "0.510318815708\n",
      "training @ iter =  122100\n",
      "0.341440320015\n",
      "training @ iter =  122200\n",
      "0.4856325984\n",
      "training @ iter =  122300\n",
      "0.351270586252\n",
      "training @ iter =  122400\n",
      "0.356939673424\n",
      "epoch 245, minibatch 500/500, test error 15.620000 %\n",
      "training @ iter =  122500\n",
      "0.599888026714\n",
      "training @ iter =  122600\n",
      "0.353567272425\n",
      "training @ iter =  122700\n",
      "0.395786583424\n",
      "training @ iter =  122800\n",
      "0.414556890726\n",
      "training @ iter =  122900\n",
      "0.350557327271\n",
      "epoch 246, minibatch 500/500, test error 15.580000 %\n",
      "training @ iter =  123000\n",
      "0.333007574081\n",
      "training @ iter =  123100\n",
      "0.301906555891\n",
      "training @ iter =  123200\n",
      "0.411701232195\n",
      "training @ iter =  123300\n",
      "0.483329534531\n",
      "training @ iter =  123400\n",
      "0.308570563793\n",
      "epoch 247, minibatch 500/500, test error 15.480000 %\n",
      "training @ iter =  123500\n",
      "0.470581352711\n",
      "training @ iter =  123600\n",
      "0.411921143532\n",
      "training @ iter =  123700\n",
      "0.405715167522\n",
      "training @ iter =  123800\n",
      "0.393546283245\n",
      "training @ iter =  123900\n",
      "0.332201659679\n",
      "epoch 248, minibatch 500/500, test error 15.530000 %\n",
      "training @ iter =  124000\n",
      "0.382332444191\n",
      "training @ iter =  124100\n",
      "0.404487073421\n",
      "training @ iter =  124200\n",
      "0.367239683867\n",
      "training @ iter =  124300\n",
      "0.351719498634\n",
      "training @ iter =  124400\n",
      "0.296619474888\n",
      "epoch 249, minibatch 500/500, test error 15.540000 %\n",
      "training @ iter =  124500\n",
      "0.494380265474\n",
      "training @ iter =  124600\n",
      "0.286529362202\n",
      "training @ iter =  124700\n",
      "0.367703855038\n",
      "training @ iter =  124800\n",
      "0.389752089977\n",
      "training @ iter =  124900\n",
      "0.299386054277\n",
      "epoch 250, minibatch 500/500, test error 15.800000 %\n",
      "training @ iter =  125000\n",
      "0.485616445541\n",
      "training @ iter =  125100\n",
      "0.338350534439\n",
      "training @ iter =  125200\n",
      "0.331224888563\n",
      "training @ iter =  125300\n",
      "0.416464149952\n",
      "training @ iter =  125400\n",
      "0.284274488688\n",
      "epoch 251, minibatch 500/500, test error 15.580000 %\n",
      "training @ iter =  125500\n",
      "0.312691837549\n",
      "training @ iter =  125600\n",
      "0.43290579319\n",
      "training @ iter =  125700\n",
      "0.364766687155\n",
      "training @ iter =  125800\n",
      "0.519337832928\n",
      "training @ iter =  125900\n",
      "0.320394128561\n",
      "epoch 252, minibatch 500/500, test error 15.550000 %\n",
      "training @ iter =  126000\n",
      "0.449555575848\n",
      "training @ iter =  126100\n",
      "0.404961615801\n",
      "training @ iter =  126200\n",
      "0.374608278275\n",
      "training @ iter =  126300\n",
      "0.422886580229\n",
      "training @ iter =  126400\n",
      "0.389971464872\n",
      "epoch 253, minibatch 500/500, test error 15.600000 %\n",
      "training @ iter =  126500\n",
      "0.355110406876\n",
      "training @ iter =  126600\n",
      "0.337464898825\n",
      "training @ iter =  126700\n",
      "0.303520470858\n",
      "training @ iter =  126800\n",
      "0.411444783211\n",
      "training @ iter =  126900\n",
      "0.330287784338\n",
      "epoch 254, minibatch 500/500, test error 15.460000 %\n",
      "training @ iter =  127000\n",
      "0.418808221817\n",
      "training @ iter =  127100\n",
      "0.449940800667\n",
      "training @ iter =  127200\n",
      "0.389360249043\n",
      "training @ iter =  127300\n",
      "0.37706643343\n",
      "training @ iter =  127400\n",
      "0.401402115822\n",
      "epoch 255, minibatch 500/500, test error 15.550000 %\n",
      "training @ iter =  127500\n",
      "0.483215868473\n",
      "training @ iter =  127600\n",
      "0.376914381981\n",
      "training @ iter =  127700\n",
      "0.3773458004\n",
      "training @ iter =  127800\n",
      "0.452294111252\n",
      "training @ iter =  127900\n",
      "0.366829991341\n",
      "epoch 256, minibatch 500/500, test error 15.360000 %\n",
      "training @ iter =  128000\n",
      "0.466473072767\n",
      "training @ iter =  128100\n",
      "0.309885412455\n",
      "training @ iter =  128200\n",
      "0.446351230145\n",
      "training @ iter =  128300\n",
      "0.469494640827\n",
      "training @ iter =  128400\n",
      "0.407579153776\n",
      "epoch 257, minibatch 500/500, test error 15.780000 %\n",
      "training @ iter =  128500\n",
      "0.406799167395\n",
      "training @ iter =  128600\n",
      "0.303030878305\n",
      "training @ iter =  128700\n",
      "0.378733932972\n",
      "training @ iter =  128800\n",
      "0.479466080666\n",
      "training @ iter =  128900\n",
      "0.366813212633\n",
      "epoch 258, minibatch 500/500, test error 15.580000 %\n",
      "training @ iter =  129000\n",
      "0.402607917786\n",
      "training @ iter =  129100\n",
      "0.231092706323\n",
      "training @ iter =  129200\n",
      "0.289856404066\n",
      "training @ iter =  129300\n",
      "0.476389765739\n",
      "training @ iter =  129400\n",
      "0.25873875618\n",
      "epoch 259, minibatch 500/500, test error 15.410000 %\n",
      "training @ iter =  129500\n",
      "0.405296206474\n",
      "training @ iter =  129600\n",
      "0.46834525466\n",
      "training @ iter =  129700\n",
      "0.392499744892\n",
      "training @ iter =  129800\n",
      "0.468073815107\n",
      "training @ iter =  129900\n",
      "0.294419914484\n",
      "epoch 260, minibatch 500/500, test error 15.770000 %\n",
      "training @ iter =  130000\n",
      "0.43573769927\n",
      "training @ iter =  130100\n",
      "0.335257649422\n",
      "training @ iter =  130200\n",
      "0.411931753159\n",
      "training @ iter =  130300\n",
      "0.48865661025\n",
      "training @ iter =  130400\n",
      "0.367548823357\n",
      "epoch 261, minibatch 500/500, test error 15.170000 %\n",
      "training @ iter =  130500\n",
      "0.445181161165\n",
      "training @ iter =  130600\n",
      "0.333107024431\n",
      "training @ iter =  130700\n",
      "0.392959028482\n",
      "training @ iter =  130800\n",
      "0.36047232151\n",
      "training @ iter =  130900\n",
      "0.416586190462\n",
      "epoch 262, minibatch 500/500, test error 15.310000 %\n",
      "training @ iter =  131000\n",
      "0.376600712538\n",
      "training @ iter =  131100\n",
      "0.377763390541\n",
      "training @ iter =  131200\n",
      "0.334890365601\n",
      "training @ iter =  131300\n",
      "0.358445882797\n",
      "training @ iter =  131400\n",
      "0.268615335226\n",
      "epoch 263, minibatch 500/500, test error 15.620000 %\n",
      "training @ iter =  131500\n",
      "0.468881607056\n",
      "training @ iter =  131600\n",
      "0.271226108074\n",
      "training @ iter =  131700\n",
      "0.312776118517\n",
      "training @ iter =  131800\n",
      "0.438013851643\n",
      "training @ iter =  131900\n",
      "0.252615749836\n",
      "epoch 264, minibatch 500/500, test error 15.440000 %\n",
      "training @ iter =  132000\n",
      "0.35775578022\n",
      "training @ iter =  132100\n",
      "0.428953409195\n",
      "training @ iter =  132200\n",
      "0.339262545109\n",
      "training @ iter =  132300\n",
      "0.455414921045\n",
      "training @ iter =  132400\n",
      "0.286089301109\n",
      "epoch 265, minibatch 500/500, test error 15.180000 %\n",
      "training @ iter =  132500\n",
      "0.320158183575\n",
      "training @ iter =  132600\n",
      "0.35606521368\n",
      "training @ iter =  132700\n",
      "0.357861787081\n",
      "training @ iter =  132800\n",
      "0.453245937824\n",
      "training @ iter =  132900\n",
      "0.306551069021\n",
      "epoch 266, minibatch 500/500, test error 15.500000 %\n",
      "training @ iter =  133000\n",
      "0.416654586792\n",
      "training @ iter =  133100\n",
      "0.340489685535\n",
      "training @ iter =  133200\n",
      "0.331427842379\n",
      "training @ iter =  133300\n",
      "0.414669960737\n",
      "training @ iter =  133400\n",
      "0.458111405373\n",
      "epoch 267, minibatch 500/500, test error 15.320000 %\n",
      "training @ iter =  133500\n",
      "0.338834911585\n",
      "training @ iter =  133600\n",
      "0.410771250725\n",
      "training @ iter =  133700\n",
      "0.396124482155\n",
      "training @ iter =  133800\n",
      "0.442334353924\n",
      "training @ iter =  133900\n",
      "0.290712714195\n",
      "epoch 268, minibatch 500/500, test error 15.330000 %\n",
      "training @ iter =  134000\n",
      "0.511962711811\n",
      "training @ iter =  134100\n",
      "0.279626190662\n",
      "training @ iter =  134200\n",
      "0.394886553288\n",
      "training @ iter =  134300\n",
      "0.402112513781\n",
      "training @ iter =  134400\n",
      "0.219368368387\n",
      "epoch 269, minibatch 500/500, test error 15.310000 %\n",
      "training @ iter =  134500\n",
      "0.353805184364\n",
      "training @ iter =  134600\n",
      "0.317185491323\n",
      "training @ iter =  134700\n",
      "0.427623093128\n",
      "training @ iter =  134800\n",
      "0.464822679758\n",
      "training @ iter =  134900\n",
      "0.373037338257\n",
      "epoch 270, minibatch 500/500, test error 15.620000 %\n",
      "training @ iter =  135000\n",
      "0.444486647844\n",
      "training @ iter =  135100\n",
      "0.478686600924\n",
      "training @ iter =  135200\n",
      "0.365377515554\n",
      "training @ iter =  135300\n",
      "0.397989243269\n",
      "training @ iter =  135400\n",
      "0.365318447351\n",
      "epoch 271, minibatch 500/500, test error 15.340000 %\n",
      "training @ iter =  135500\n",
      "0.429801017046\n",
      "training @ iter =  135600\n",
      "0.27310320735\n",
      "training @ iter =  135700\n",
      "0.350342154503\n",
      "training @ iter =  135800\n",
      "0.343270123005\n",
      "training @ iter =  135900\n",
      "0.328975439072\n",
      "epoch 272, minibatch 500/500, test error 15.270000 %\n",
      "training @ iter =  136000\n",
      "0.460267782211\n",
      "training @ iter =  136100\n",
      "0.284173518419\n",
      "training @ iter =  136200\n",
      "0.435935199261\n",
      "training @ iter =  136300\n",
      "0.445788681507\n",
      "training @ iter =  136400\n",
      "0.36194434762\n",
      "epoch 273, minibatch 500/500, test error 15.390000 %\n",
      "training @ iter =  136500\n",
      "0.3833963871\n",
      "training @ iter =  136600\n",
      "0.348039925098\n",
      "training @ iter =  136700\n",
      "0.370850116014\n",
      "training @ iter =  136800\n",
      "0.387393712997\n",
      "training @ iter =  136900\n",
      "0.319712400436\n",
      "epoch 274, minibatch 500/500, test error 15.230000 %\n",
      "training @ iter =  137000\n",
      "0.403037577868\n",
      "training @ iter =  137100\n",
      "0.385955840349\n",
      "training @ iter =  137200\n",
      "0.337124437094\n",
      "training @ iter =  137300\n",
      "0.359013974667\n",
      "training @ iter =  137400\n",
      "0.373727560043\n",
      "epoch 275, minibatch 500/500, test error 15.150000 %\n",
      "training @ iter =  137500\n",
      "0.386891931295\n",
      "training @ iter =  137600\n",
      "0.343674510717\n",
      "training @ iter =  137700\n",
      "0.304583191872\n",
      "training @ iter =  137800\n",
      "0.385786324739\n",
      "training @ iter =  137900\n",
      "0.463485002518\n",
      "epoch 276, minibatch 500/500, test error 15.430000 %\n",
      "training @ iter =  138000\n",
      "0.393038094044\n",
      "training @ iter =  138100\n",
      "0.36434173584\n",
      "training @ iter =  138200\n",
      "0.273172616959\n",
      "training @ iter =  138300\n",
      "0.458729356527\n",
      "training @ iter =  138400\n",
      "0.380875766277\n",
      "epoch 277, minibatch 500/500, test error 15.020000 %\n",
      "training @ iter =  138500\n",
      "0.479747623205\n",
      "training @ iter =  138600\n",
      "0.370087057352\n",
      "training @ iter =  138700\n",
      "0.379993587732\n",
      "training @ iter =  138800\n",
      "0.383706122637\n",
      "training @ iter =  138900\n",
      "0.328846514225\n",
      "epoch 278, minibatch 500/500, test error 15.060000 %\n",
      "training @ iter =  139000\n",
      "0.483755648136\n",
      "training @ iter =  139100\n",
      "0.405747294426\n",
      "training @ iter =  139200\n",
      "0.309850335121\n",
      "training @ iter =  139300\n",
      "0.417169958353\n",
      "training @ iter =  139400\n",
      "0.336184382439\n",
      "epoch 279, minibatch 500/500, test error 15.470000 %\n",
      "training @ iter =  139500\n",
      "0.434566885233\n",
      "training @ iter =  139600\n",
      "0.315957933664\n",
      "training @ iter =  139700\n",
      "0.374156981707\n",
      "training @ iter =  139800\n",
      "0.41298365593\n",
      "training @ iter =  139900\n",
      "0.355955809355\n",
      "epoch 280, minibatch 500/500, test error 15.150000 %\n",
      "training @ iter =  140000\n",
      "0.361545115709\n",
      "training @ iter =  140100\n",
      "0.255131751299\n",
      "training @ iter =  140200\n",
      "0.367534279823\n",
      "training @ iter =  140300\n",
      "0.387736827135\n",
      "training @ iter =  140400\n",
      "0.347752809525\n",
      "epoch 281, minibatch 500/500, test error 15.400000 %\n",
      "training @ iter =  140500\n",
      "0.390211552382\n",
      "training @ iter =  140600\n",
      "0.311449974775\n",
      "training @ iter =  140700\n",
      "0.403952628374\n",
      "training @ iter =  140800\n",
      "0.423950195312\n",
      "training @ iter =  140900\n",
      "0.196916162968\n",
      "epoch 282, minibatch 500/500, test error 15.190000 %\n",
      "training @ iter =  141000\n",
      "0.436427772045\n",
      "training @ iter =  141100\n",
      "0.364434063435\n",
      "training @ iter =  141200\n",
      "0.326075434685\n",
      "training @ iter =  141300\n",
      "0.382320046425\n",
      "training @ iter =  141400\n",
      "0.332712262869\n",
      "epoch 283, minibatch 500/500, test error 15.200000 %\n",
      "training @ iter =  141500\n",
      "0.439655393362\n",
      "training @ iter =  141600\n",
      "0.320363432169\n",
      "training @ iter =  141700\n",
      "0.339030563831\n",
      "training @ iter =  141800\n",
      "0.404573351145\n",
      "training @ iter =  141900\n",
      "0.430614382029\n",
      "epoch 284, minibatch 500/500, test error 15.140000 %\n",
      "training @ iter =  142000\n",
      "0.527212560177\n",
      "training @ iter =  142100\n",
      "0.335386574268\n",
      "training @ iter =  142200\n",
      "0.4270208776\n",
      "training @ iter =  142300\n",
      "0.456334143877\n",
      "training @ iter =  142400\n",
      "0.29538577795\n",
      "epoch 285, minibatch 500/500, test error 15.300000 %\n",
      "training @ iter =  142500\n",
      "0.440159380436\n",
      "training @ iter =  142600\n",
      "0.290553539991\n",
      "training @ iter =  142700\n",
      "0.403268545866\n",
      "training @ iter =  142800\n",
      "0.377290457487\n",
      "training @ iter =  142900\n",
      "0.358718335629\n",
      "epoch 286, minibatch 500/500, test error 15.540000 %\n",
      "training @ iter =  143000\n",
      "0.319672703743\n",
      "training @ iter =  143100\n",
      "0.380616158247\n",
      "training @ iter =  143200\n",
      "0.32083773613\n",
      "training @ iter =  143300\n",
      "0.514790177345\n",
      "training @ iter =  143400\n",
      "0.299529731274\n",
      "epoch 287, minibatch 500/500, test error 15.090000 %\n",
      "training @ iter =  143500\n",
      "0.530161738396\n",
      "training @ iter =  143600\n",
      "0.290223151445\n",
      "training @ iter =  143700\n",
      "0.347455203533\n",
      "training @ iter =  143800\n",
      "0.502227962017\n",
      "training @ iter =  143900\n",
      "0.27826833725\n",
      "epoch 288, minibatch 500/500, test error 15.130000 %\n",
      "training @ iter =  144000\n",
      "0.310510724783\n",
      "training @ iter =  144100\n",
      "0.258701980114\n",
      "training @ iter =  144200\n",
      "0.298391014338\n",
      "training @ iter =  144300\n",
      "0.380656003952\n",
      "training @ iter =  144400\n",
      "0.300497293472\n",
      "epoch 289, minibatch 500/500, test error 15.160000 %\n",
      "training @ iter =  144500\n",
      "0.499733954668\n",
      "training @ iter =  144600\n",
      "0.328616678715\n",
      "training @ iter =  144700\n",
      "0.420084685087\n",
      "training @ iter =  144800\n",
      "0.387741625309\n",
      "training @ iter =  144900\n",
      "0.364006578922\n",
      "epoch 290, minibatch 500/500, test error 14.960000 %\n",
      "training @ iter =  145000\n",
      "0.328126311302\n",
      "training @ iter =  145100\n",
      "0.384330034256\n",
      "training @ iter =  145200\n",
      "0.465967088938\n",
      "training @ iter =  145300\n",
      "0.34822037816\n",
      "training @ iter =  145400\n",
      "0.270306885242\n",
      "epoch 291, minibatch 500/500, test error 15.110000 %\n",
      "training @ iter =  145500\n",
      "0.359900355339\n",
      "training @ iter =  145600\n",
      "0.418697863817\n",
      "training @ iter =  145700\n",
      "0.353543162346\n",
      "training @ iter =  145800\n",
      "0.308012843132\n",
      "training @ iter =  145900\n",
      "0.301623731852\n",
      "epoch 292, minibatch 500/500, test error 15.310000 %\n",
      "training @ iter =  146000\n",
      "0.390247732401\n",
      "training @ iter =  146100\n",
      "0.382939219475\n",
      "training @ iter =  146200\n",
      "0.308901280165\n",
      "training @ iter =  146300\n",
      "0.405555456877\n",
      "training @ iter =  146400\n",
      "0.315161168575\n",
      "epoch 293, minibatch 500/500, test error 15.500000 %\n",
      "training @ iter =  146500\n",
      "0.322043448687\n",
      "training @ iter =  146600\n",
      "0.212691798806\n",
      "training @ iter =  146700\n",
      "0.43818116188\n",
      "training @ iter =  146800\n",
      "0.365353882313\n",
      "training @ iter =  146900\n",
      "0.325917482376\n",
      "epoch 294, minibatch 500/500, test error 15.190000 %\n",
      "training @ iter =  147000\n",
      "0.440372735262\n",
      "training @ iter =  147100\n",
      "0.362087398767\n",
      "training @ iter =  147200\n",
      "0.324278861284\n",
      "training @ iter =  147300\n",
      "0.391694456339\n",
      "training @ iter =  147400\n",
      "0.340944975615\n",
      "epoch 295, minibatch 500/500, test error 15.250000 %\n",
      "training @ iter =  147500\n",
      "0.386523157358\n",
      "training @ iter =  147600\n",
      "0.350547641516\n",
      "training @ iter =  147700\n",
      "0.257783055305\n",
      "training @ iter =  147800\n",
      "0.404971152544\n",
      "training @ iter =  147900\n",
      "0.32139903307\n",
      "epoch 296, minibatch 500/500, test error 15.010000 %\n",
      "training @ iter =  148000\n",
      "0.315280079842\n",
      "training @ iter =  148100\n",
      "0.395051807165\n",
      "training @ iter =  148200\n",
      "0.412133336067\n",
      "training @ iter =  148300\n",
      "0.328644305468\n",
      "training @ iter =  148400\n",
      "0.318175762892\n",
      "epoch 297, minibatch 500/500, test error 15.100000 %\n",
      "training @ iter =  148500\n",
      "0.371945798397\n",
      "training @ iter =  148600\n",
      "0.306697487831\n",
      "training @ iter =  148700\n",
      "0.354570388794\n",
      "training @ iter =  148800\n",
      "0.37885338068\n",
      "training @ iter =  148900\n",
      "0.453755915165\n",
      "epoch 298, minibatch 500/500, test error 15.220000 %\n",
      "training @ iter =  149000\n",
      "0.384304314852\n",
      "training @ iter =  149100\n",
      "0.314492046833\n",
      "training @ iter =  149200\n",
      "0.315580248833\n",
      "training @ iter =  149300\n",
      "0.369890600443\n",
      "training @ iter =  149400\n",
      "0.238928839564\n",
      "epoch 299, minibatch 500/500, test error 14.900000 %\n",
      "training @ iter =  149500\n",
      "0.425460666418\n",
      "training @ iter =  149600\n",
      "0.381026297808\n",
      "training @ iter =  149700\n",
      "0.39587187767\n",
      "training @ iter =  149800\n",
      "0.481345683336\n",
      "training @ iter =  149900\n",
      "0.181309089065\n",
      "epoch 300, minibatch 500/500, test error 15.100000 %\n",
      "training @ iter =  150000\n",
      "0.498876571655\n",
      "training @ iter =  150100\n",
      "0.38791179657\n",
      "training @ iter =  150200\n",
      "0.265444308519\n",
      "training @ iter =  150300\n",
      "0.327814012766\n",
      "training @ iter =  150400\n",
      "0.314908564091\n",
      "epoch 301, minibatch 500/500, test error 14.870000 %\n",
      "training @ iter =  150500\n",
      "0.328567951918\n",
      "training @ iter =  150600\n",
      "0.369631886482\n",
      "training @ iter =  150700\n",
      "0.357438802719\n",
      "training @ iter =  150800\n",
      "0.439403891563\n",
      "training @ iter =  150900\n",
      "0.371747851372\n",
      "epoch 302, minibatch 500/500, test error 14.830000 %\n",
      "training @ iter =  151000\n",
      "0.412145763636\n",
      "training @ iter =  151100\n",
      "0.31916809082\n",
      "training @ iter =  151200\n",
      "0.247574403882\n",
      "training @ iter =  151300\n",
      "0.337443321943\n",
      "training @ iter =  151400\n",
      "0.349193751812\n",
      "epoch 303, minibatch 500/500, test error 14.610000 %\n",
      "training @ iter =  151500\n",
      "0.435745388269\n",
      "training @ iter =  151600\n",
      "0.388315349817\n",
      "training @ iter =  151700\n",
      "0.311523139477\n",
      "training @ iter =  151800\n",
      "0.384955763817\n",
      "training @ iter =  151900\n",
      "0.338905751705\n",
      "epoch 304, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  152000\n",
      "0.371985524893\n",
      "training @ iter =  152100\n",
      "0.243355900049\n",
      "training @ iter =  152200\n",
      "0.294200539589\n",
      "training @ iter =  152300\n",
      "0.465375810862\n",
      "training @ iter =  152400\n",
      "0.230158552527\n",
      "epoch 305, minibatch 500/500, test error 14.680000 %\n",
      "training @ iter =  152500\n",
      "0.433513104916\n",
      "training @ iter =  152600\n",
      "0.286516278982\n",
      "training @ iter =  152700\n",
      "0.364108264446\n",
      "training @ iter =  152800\n",
      "0.364424884319\n",
      "training @ iter =  152900\n",
      "0.361636817455\n",
      "epoch 306, minibatch 500/500, test error 14.560000 %\n",
      "training @ iter =  153000\n",
      "0.376160442829\n",
      "training @ iter =  153100\n",
      "0.301619917154\n",
      "training @ iter =  153200\n",
      "0.315904319286\n",
      "training @ iter =  153300\n",
      "0.369612157345\n",
      "training @ iter =  153400\n",
      "0.247508168221\n",
      "epoch 307, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  153500\n",
      "0.480703175068\n",
      "training @ iter =  153600\n",
      "0.365744441748\n",
      "training @ iter =  153700\n",
      "0.343141466379\n",
      "training @ iter =  153800\n",
      "0.376980930567\n",
      "training @ iter =  153900\n",
      "0.282465338707\n",
      "epoch 308, minibatch 500/500, test error 14.750000 %\n",
      "training @ iter =  154000\n",
      "0.346971273422\n",
      "training @ iter =  154100\n",
      "0.327089160681\n",
      "training @ iter =  154200\n",
      "0.379606693983\n",
      "training @ iter =  154300\n",
      "0.377575010061\n",
      "training @ iter =  154400\n",
      "0.394899100065\n",
      "epoch 309, minibatch 500/500, test error 14.760000 %\n",
      "training @ iter =  154500\n",
      "0.319000244141\n",
      "training @ iter =  154600\n",
      "0.380455255508\n",
      "training @ iter =  154700\n",
      "0.281219094992\n",
      "training @ iter =  154800\n",
      "0.383847653866\n",
      "training @ iter =  154900\n",
      "0.271522909403\n",
      "epoch 310, minibatch 500/500, test error 14.870000 %\n",
      "training @ iter =  155000\n",
      "0.400642901659\n",
      "training @ iter =  155100\n",
      "0.380272179842\n",
      "training @ iter =  155200\n",
      "0.32887172699\n",
      "training @ iter =  155300\n",
      "0.371523976326\n",
      "training @ iter =  155400\n",
      "0.217811778188\n",
      "epoch 311, minibatch 500/500, test error 14.800000 %\n",
      "training @ iter =  155500\n",
      "0.53902566433\n",
      "training @ iter =  155600\n",
      "0.370637774467\n",
      "training @ iter =  155700\n",
      "0.329261958599\n",
      "training @ iter =  155800\n",
      "0.375053673983\n",
      "training @ iter =  155900\n",
      "0.456564486027\n",
      "epoch 312, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  156000\n",
      "0.401879489422\n",
      "training @ iter =  156100\n",
      "0.397642284632\n",
      "training @ iter =  156200\n",
      "0.295232415199\n",
      "training @ iter =  156300\n",
      "0.371483683586\n",
      "training @ iter =  156400\n",
      "0.306965142488\n",
      "epoch 313, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  156500\n",
      "0.356402426958\n",
      "training @ iter =  156600\n",
      "0.308098256588\n",
      "training @ iter =  156700\n",
      "0.267141252756\n",
      "training @ iter =  156800\n",
      "0.419482380152\n",
      "training @ iter =  156900\n",
      "0.35676074028\n",
      "epoch 314, minibatch 500/500, test error 14.780000 %\n",
      "training @ iter =  157000\n",
      "0.391536414623\n",
      "training @ iter =  157100\n",
      "0.284386724234\n",
      "training @ iter =  157200\n",
      "0.421539068222\n",
      "training @ iter =  157300\n",
      "0.421302765608\n",
      "training @ iter =  157400\n",
      "0.287521392107\n",
      "epoch 315, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  157500\n",
      "0.389566779137\n",
      "training @ iter =  157600\n",
      "0.306487768888\n",
      "training @ iter =  157700\n",
      "0.323574364185\n",
      "training @ iter =  157800\n",
      "0.433598130941\n",
      "training @ iter =  157900\n",
      "0.335360527039\n",
      "epoch 316, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  158000\n",
      "0.392672032118\n",
      "training @ iter =  158100\n",
      "0.303875923157\n",
      "training @ iter =  158200\n",
      "0.299557924271\n",
      "training @ iter =  158300\n",
      "0.338462680578\n",
      "training @ iter =  158400\n",
      "0.308068037033\n",
      "epoch 317, minibatch 500/500, test error 14.540000 %\n",
      "training @ iter =  158500\n",
      "0.362266033888\n",
      "training @ iter =  158600\n",
      "0.311172664165\n",
      "training @ iter =  158700\n",
      "0.317593961954\n",
      "training @ iter =  158800\n",
      "0.497392058372\n",
      "training @ iter =  158900\n",
      "0.284135639668\n",
      "epoch 318, minibatch 500/500, test error 14.780000 %\n",
      "training @ iter =  159000\n",
      "0.375501990318\n",
      "training @ iter =  159100\n",
      "0.314006060362\n",
      "training @ iter =  159200\n",
      "0.353369623423\n",
      "training @ iter =  159300\n",
      "0.398715049028\n",
      "training @ iter =  159400\n",
      "0.348147213459\n",
      "epoch 319, minibatch 500/500, test error 14.790000 %\n",
      "training @ iter =  159500\n",
      "0.323761373758\n",
      "training @ iter =  159600\n",
      "0.311036974192\n",
      "training @ iter =  159700\n",
      "0.29600045085\n",
      "training @ iter =  159800\n",
      "0.350388348103\n",
      "training @ iter =  159900\n",
      "0.258752793074\n",
      "epoch 320, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  160000\n",
      "0.354272991419\n",
      "training @ iter =  160100\n",
      "0.254137217999\n",
      "training @ iter =  160200\n",
      "0.308169960976\n",
      "training @ iter =  160300\n",
      "0.476635664701\n",
      "training @ iter =  160400\n",
      "0.393192529678\n",
      "epoch 321, minibatch 500/500, test error 14.680000 %\n",
      "training @ iter =  160500\n",
      "0.338848561049\n",
      "training @ iter =  160600\n",
      "0.259665518999\n",
      "training @ iter =  160700\n",
      "0.334726035595\n",
      "training @ iter =  160800\n",
      "0.327855527401\n",
      "training @ iter =  160900\n",
      "0.332129359245\n",
      "epoch 322, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  161000\n",
      "0.423990100622\n",
      "training @ iter =  161100\n",
      "0.257585763931\n",
      "training @ iter =  161200\n",
      "0.237963154912\n",
      "training @ iter =  161300\n",
      "0.502929091454\n",
      "training @ iter =  161400\n",
      "0.301909506321\n",
      "epoch 323, minibatch 500/500, test error 14.620000 %\n",
      "training @ iter =  161500\n",
      "0.253540575504\n",
      "training @ iter =  161600\n",
      "0.314016342163\n",
      "training @ iter =  161700\n",
      "0.261330902576\n",
      "training @ iter =  161800\n",
      "0.380619883537\n",
      "training @ iter =  161900\n",
      "0.264097183943\n",
      "epoch 324, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  162000\n",
      "0.456794649363\n",
      "training @ iter =  162100\n",
      "0.369800388813\n",
      "training @ iter =  162200\n",
      "0.226123079658\n",
      "training @ iter =  162300\n",
      "0.331919401884\n",
      "training @ iter =  162400\n",
      "0.269563466311\n",
      "epoch 325, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  162500\n",
      "0.371763527393\n",
      "training @ iter =  162600\n",
      "0.387909859419\n",
      "training @ iter =  162700\n",
      "0.29780036211\n",
      "training @ iter =  162800\n",
      "0.391352266073\n",
      "training @ iter =  162900\n",
      "0.284129321575\n",
      "epoch 326, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  163000\n",
      "0.36083611846\n",
      "training @ iter =  163100\n",
      "0.355078458786\n",
      "training @ iter =  163200\n",
      "0.261027753353\n",
      "training @ iter =  163300\n",
      "0.346598923206\n",
      "training @ iter =  163400\n",
      "0.291200518608\n",
      "epoch 327, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  163500\n",
      "0.388378560543\n",
      "training @ iter =  163600\n",
      "0.364850699902\n",
      "training @ iter =  163700\n",
      "0.402192771435\n",
      "training @ iter =  163800\n",
      "0.315621614456\n",
      "training @ iter =  163900\n",
      "0.263528972864\n",
      "epoch 328, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  164000\n",
      "0.467821091413\n",
      "training @ iter =  164100\n",
      "0.30002990365\n",
      "training @ iter =  164200\n",
      "0.391786724329\n",
      "training @ iter =  164300\n",
      "0.359725177288\n",
      "training @ iter =  164400\n",
      "0.289287567139\n",
      "epoch 329, minibatch 500/500, test error 14.560000 %\n",
      "training @ iter =  164500\n",
      "0.370383232832\n",
      "training @ iter =  164600\n",
      "0.320048779249\n",
      "training @ iter =  164700\n",
      "0.340230852365\n",
      "training @ iter =  164800\n",
      "0.425502121449\n",
      "training @ iter =  164900\n",
      "0.189407765865\n",
      "epoch 330, minibatch 500/500, test error 14.600000 %\n",
      "training @ iter =  165000\n",
      "0.444060355425\n",
      "training @ iter =  165100\n",
      "0.293929606676\n",
      "training @ iter =  165200\n",
      "0.338633090258\n",
      "training @ iter =  165300\n",
      "0.413819283247\n",
      "training @ iter =  165400\n",
      "0.363209336996\n",
      "epoch 331, minibatch 500/500, test error 14.500000 %\n",
      "training @ iter =  165500\n",
      "0.344377219677\n",
      "training @ iter =  165600\n",
      "0.332927703857\n",
      "training @ iter =  165700\n",
      "0.374574899673\n",
      "training @ iter =  165800\n",
      "0.313024431467\n",
      "training @ iter =  165900\n",
      "0.372004151344\n",
      "epoch 332, minibatch 500/500, test error 14.710000 %\n",
      "training @ iter =  166000\n",
      "0.393606752157\n",
      "training @ iter =  166100\n",
      "0.296647191048\n",
      "training @ iter =  166200\n",
      "0.283487021923\n",
      "training @ iter =  166300\n",
      "0.301585882902\n",
      "training @ iter =  166400\n",
      "0.353727191687\n",
      "epoch 333, minibatch 500/500, test error 14.820000 %\n",
      "training @ iter =  166500\n",
      "0.297864675522\n",
      "training @ iter =  166600\n",
      "0.389260679483\n",
      "training @ iter =  166700\n",
      "0.295609116554\n",
      "training @ iter =  166800\n",
      "0.401564776897\n",
      "training @ iter =  166900\n",
      "0.277120769024\n",
      "epoch 334, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  167000\n",
      "0.384080588818\n",
      "training @ iter =  167100\n",
      "0.219242826104\n",
      "training @ iter =  167200\n",
      "0.352690964937\n",
      "training @ iter =  167300\n",
      "0.412927269936\n",
      "training @ iter =  167400\n",
      "0.261069744825\n",
      "epoch 335, minibatch 500/500, test error 14.570000 %\n",
      "training @ iter =  167500\n",
      "0.301101535559\n",
      "training @ iter =  167600\n",
      "0.287371248007\n",
      "training @ iter =  167700\n",
      "0.331045925617\n",
      "training @ iter =  167800\n",
      "0.429968595505\n",
      "training @ iter =  167900\n",
      "0.333880603313\n",
      "epoch 336, minibatch 500/500, test error 14.500000 %\n",
      "training @ iter =  168000\n",
      "0.39536216855\n",
      "training @ iter =  168100\n",
      "0.357707053423\n",
      "training @ iter =  168200\n",
      "0.212675705552\n",
      "training @ iter =  168300\n",
      "0.361135363579\n",
      "training @ iter =  168400\n",
      "0.251064777374\n",
      "epoch 337, minibatch 500/500, test error 14.590000 %\n",
      "training @ iter =  168500\n",
      "0.446441739798\n",
      "training @ iter =  168600\n",
      "0.309651434422\n",
      "training @ iter =  168700\n",
      "0.347252041101\n",
      "training @ iter =  168800\n",
      "0.420569688082\n",
      "training @ iter =  168900\n",
      "0.220537483692\n",
      "epoch 338, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  169000\n",
      "0.306184411049\n",
      "training @ iter =  169100\n",
      "0.341722667217\n",
      "training @ iter =  169200\n",
      "0.339109510183\n",
      "training @ iter =  169300\n",
      "0.383092492819\n",
      "training @ iter =  169400\n",
      "0.319985687733\n",
      "epoch 339, minibatch 500/500, test error 14.600000 %\n",
      "training @ iter =  169500\n",
      "0.467457056046\n",
      "training @ iter =  169600\n",
      "0.215079903603\n",
      "training @ iter =  169700\n",
      "0.315479308367\n",
      "training @ iter =  169800\n",
      "0.353910803795\n",
      "training @ iter =  169900\n",
      "0.334866166115\n",
      "epoch 340, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  170000\n",
      "0.320603787899\n",
      "training @ iter =  170100\n",
      "0.349181354046\n",
      "training @ iter =  170200\n",
      "0.245717167854\n",
      "training @ iter =  170300\n",
      "0.368041694164\n",
      "training @ iter =  170400\n",
      "0.281815171242\n",
      "epoch 341, minibatch 500/500, test error 14.570000 %\n",
      "training @ iter =  170500\n",
      "0.312639951706\n",
      "training @ iter =  170600\n",
      "0.28318336606\n",
      "training @ iter =  170700\n",
      "0.348119139671\n",
      "training @ iter =  170800\n",
      "0.389665871859\n",
      "training @ iter =  170900\n",
      "0.444764584303\n",
      "epoch 342, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  171000\n",
      "0.299784958363\n",
      "training @ iter =  171100\n",
      "0.339878857136\n",
      "training @ iter =  171200\n",
      "0.342236608267\n",
      "training @ iter =  171300\n",
      "0.369238257408\n",
      "training @ iter =  171400\n",
      "0.275875896215\n",
      "epoch 343, minibatch 500/500, test error 14.750000 %\n",
      "training @ iter =  171500\n",
      "0.392522215843\n",
      "training @ iter =  171600\n",
      "0.242453232408\n",
      "training @ iter =  171700\n",
      "0.302865624428\n",
      "training @ iter =  171800\n",
      "0.391168147326\n",
      "training @ iter =  171900\n",
      "0.322067141533\n",
      "epoch 344, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  172000\n",
      "0.462085068226\n",
      "training @ iter =  172100\n",
      "0.288925081491\n",
      "training @ iter =  172200\n",
      "0.307669252157\n",
      "training @ iter =  172300\n",
      "0.454139828682\n",
      "training @ iter =  172400\n",
      "0.241140782833\n",
      "epoch 345, minibatch 500/500, test error 14.800000 %\n",
      "training @ iter =  172500\n",
      "0.361407250166\n",
      "training @ iter =  172600\n",
      "0.299945175648\n",
      "training @ iter =  172700\n",
      "0.460942983627\n",
      "training @ iter =  172800\n",
      "0.400693446398\n",
      "training @ iter =  172900\n",
      "0.273966670036\n",
      "epoch 346, minibatch 500/500, test error 14.770000 %\n",
      "training @ iter =  173000\n",
      "0.364067345858\n",
      "training @ iter =  173100\n",
      "0.355339735746\n",
      "training @ iter =  173200\n",
      "0.319989115\n",
      "training @ iter =  173300\n",
      "0.382338017225\n",
      "training @ iter =  173400\n",
      "0.218112483621\n",
      "epoch 347, minibatch 500/500, test error 14.780000 %\n",
      "training @ iter =  173500\n",
      "0.396832734346\n",
      "training @ iter =  173600\n",
      "0.253582417965\n",
      "training @ iter =  173700\n",
      "0.267048120499\n",
      "training @ iter =  173800\n",
      "0.351222068071\n",
      "training @ iter =  173900\n",
      "0.266096889973\n",
      "epoch 348, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  174000\n",
      "0.37002953887\n",
      "training @ iter =  174100\n",
      "0.27083709836\n",
      "training @ iter =  174200\n",
      "0.254782795906\n",
      "training @ iter =  174300\n",
      "0.360326528549\n",
      "training @ iter =  174400\n",
      "0.264328211546\n",
      "epoch 349, minibatch 500/500, test error 14.570000 %\n",
      "training @ iter =  174500\n",
      "0.402265936136\n",
      "training @ iter =  174600\n",
      "0.352854967117\n",
      "training @ iter =  174700\n",
      "0.383884578943\n",
      "training @ iter =  174800\n",
      "0.353517025709\n",
      "training @ iter =  174900\n",
      "0.280587553978\n",
      "epoch 350, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  175000\n",
      "0.414667665958\n",
      "training @ iter =  175100\n",
      "0.340710282326\n",
      "training @ iter =  175200\n",
      "0.333813250065\n",
      "training @ iter =  175300\n",
      "0.387207955122\n",
      "training @ iter =  175400\n",
      "0.327266931534\n",
      "epoch 351, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  175500\n",
      "0.348477810621\n",
      "training @ iter =  175600\n",
      "0.282059788704\n",
      "training @ iter =  175700\n",
      "0.310864955187\n",
      "training @ iter =  175800\n",
      "0.390964210033\n",
      "training @ iter =  175900\n",
      "0.26665520668\n",
      "epoch 352, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  176000\n",
      "0.355395287275\n",
      "training @ iter =  176100\n",
      "0.369053125381\n",
      "training @ iter =  176200\n",
      "0.287498354912\n",
      "training @ iter =  176300\n",
      "0.362461775541\n",
      "training @ iter =  176400\n",
      "0.326623260975\n",
      "epoch 353, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  176500\n",
      "0.343787461519\n",
      "training @ iter =  176600\n",
      "0.37109169364\n",
      "training @ iter =  176700\n",
      "0.330783843994\n",
      "training @ iter =  176800\n",
      "0.450527757406\n",
      "training @ iter =  176900\n",
      "0.252664744854\n",
      "epoch 354, minibatch 500/500, test error 14.740000 %\n",
      "training @ iter =  177000\n",
      "0.314369291067\n",
      "training @ iter =  177100\n",
      "0.377999544144\n",
      "training @ iter =  177200\n",
      "0.337505251169\n",
      "training @ iter =  177300\n",
      "0.46806794405\n",
      "training @ iter =  177400\n",
      "0.217182919383\n",
      "epoch 355, minibatch 500/500, test error 14.750000 %\n",
      "training @ iter =  177500\n",
      "0.384134173393\n",
      "training @ iter =  177600\n",
      "0.272228419781\n",
      "training @ iter =  177700\n",
      "0.320309072733\n",
      "training @ iter =  177800\n",
      "0.390138536692\n",
      "training @ iter =  177900\n",
      "0.263611197472\n",
      "epoch 356, minibatch 500/500, test error 14.720000 %\n",
      "training @ iter =  178000\n",
      "0.390320181847\n",
      "training @ iter =  178100\n",
      "0.346449583769\n",
      "training @ iter =  178200\n",
      "0.249959141016\n",
      "training @ iter =  178300\n",
      "0.335098564625\n",
      "training @ iter =  178400\n",
      "0.329933464527\n",
      "epoch 357, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  178500\n",
      "0.284406274557\n",
      "training @ iter =  178600\n",
      "0.289616703987\n",
      "training @ iter =  178700\n",
      "0.399408608675\n",
      "training @ iter =  178800\n",
      "0.267461389303\n",
      "training @ iter =  178900\n",
      "0.23353561759\n",
      "epoch 358, minibatch 500/500, test error 14.710000 %\n",
      "training @ iter =  179000\n",
      "0.38527327776\n",
      "training @ iter =  179100\n",
      "0.299036353827\n",
      "training @ iter =  179200\n",
      "0.34081402421\n",
      "training @ iter =  179300\n",
      "0.453214943409\n",
      "training @ iter =  179400\n",
      "0.315732657909\n",
      "epoch 359, minibatch 500/500, test error 14.650000 %\n",
      "training @ iter =  179500\n",
      "0.282577335835\n",
      "training @ iter =  179600\n",
      "0.324353814125\n",
      "training @ iter =  179700\n",
      "0.287960022688\n",
      "training @ iter =  179800\n",
      "0.418707668781\n",
      "training @ iter =  179900\n",
      "0.386788874865\n",
      "epoch 360, minibatch 500/500, test error 14.630000 %\n",
      "training @ iter =  180000\n",
      "0.345101505518\n",
      "training @ iter =  180100\n",
      "0.337371826172\n",
      "training @ iter =  180200\n",
      "0.278263062239\n",
      "training @ iter =  180300\n",
      "0.377238661051\n",
      "training @ iter =  180400\n",
      "0.271783381701\n",
      "epoch 361, minibatch 500/500, test error 14.680000 %\n",
      "training @ iter =  180500\n",
      "0.353985518217\n",
      "training @ iter =  180600\n",
      "0.242859572172\n",
      "training @ iter =  180700\n",
      "0.275265306234\n",
      "training @ iter =  180800\n",
      "0.323943406343\n",
      "training @ iter =  180900\n",
      "0.259040921926\n",
      "epoch 362, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  181000\n",
      "0.429917991161\n",
      "training @ iter =  181100\n",
      "0.329336613417\n",
      "training @ iter =  181200\n",
      "0.441429674625\n",
      "training @ iter =  181300\n",
      "0.413269072771\n",
      "training @ iter =  181400\n",
      "0.271395325661\n",
      "epoch 363, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  181500\n",
      "0.377663433552\n",
      "training @ iter =  181600\n",
      "0.328366458416\n",
      "training @ iter =  181700\n",
      "0.40796661377\n",
      "training @ iter =  181800\n",
      "0.312544792891\n",
      "training @ iter =  181900\n",
      "0.258486837149\n",
      "epoch 364, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  182000\n",
      "0.292355149984\n",
      "training @ iter =  182100\n",
      "0.297633588314\n",
      "training @ iter =  182200\n",
      "0.339075535536\n",
      "training @ iter =  182300\n",
      "0.417412787676\n",
      "training @ iter =  182400\n",
      "0.317170143127\n",
      "epoch 365, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  182500\n",
      "0.325577616692\n",
      "training @ iter =  182600\n",
      "0.318974405527\n",
      "training @ iter =  182700\n",
      "0.410921096802\n",
      "training @ iter =  182800\n",
      "0.305842995644\n",
      "training @ iter =  182900\n",
      "0.265613436699\n",
      "epoch 366, minibatch 500/500, test error 14.720000 %\n",
      "training @ iter =  183000\n",
      "0.404773265123\n",
      "training @ iter =  183100\n",
      "0.340376675129\n",
      "training @ iter =  183200\n",
      "0.415688931942\n",
      "training @ iter =  183300\n",
      "0.350406914949\n",
      "training @ iter =  183400\n",
      "0.420947164297\n",
      "epoch 367, minibatch 500/500, test error 14.610000 %\n",
      "training @ iter =  183500\n",
      "0.336471945047\n",
      "training @ iter =  183600\n",
      "0.294590175152\n",
      "training @ iter =  183700\n",
      "0.338501244783\n",
      "training @ iter =  183800\n",
      "0.437745124102\n",
      "training @ iter =  183900\n",
      "0.403854757547\n",
      "epoch 368, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  184000\n",
      "0.325744479895\n",
      "training @ iter =  184100\n",
      "0.263306498528\n",
      "training @ iter =  184200\n",
      "0.324051499367\n",
      "training @ iter =  184300\n",
      "0.430149763823\n",
      "training @ iter =  184400\n",
      "0.359709918499\n",
      "epoch 369, minibatch 500/500, test error 14.630000 %\n",
      "training @ iter =  184500\n",
      "0.363092690706\n",
      "training @ iter =  184600\n",
      "0.249871075153\n",
      "training @ iter =  184700\n",
      "0.319564402103\n",
      "training @ iter =  184800\n",
      "0.486349821091\n",
      "training @ iter =  184900\n",
      "0.27889212966\n",
      "epoch 370, minibatch 500/500, test error 14.640000 %\n",
      "training @ iter =  185000\n",
      "0.443702816963\n",
      "training @ iter =  185100\n",
      "0.319462686777\n",
      "training @ iter =  185200\n",
      "0.271608054638\n",
      "training @ iter =  185300\n",
      "0.294025063515\n",
      "training @ iter =  185400\n",
      "0.267653942108\n",
      "epoch 371, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  185500\n",
      "0.436778455973\n",
      "training @ iter =  185600\n",
      "0.274323970079\n",
      "training @ iter =  185700\n",
      "0.361151814461\n",
      "training @ iter =  185800\n",
      "0.459447979927\n",
      "training @ iter =  185900\n",
      "0.209342688322\n",
      "epoch 372, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  186000\n",
      "0.352366745472\n",
      "training @ iter =  186100\n",
      "0.325978666544\n",
      "training @ iter =  186200\n",
      "0.316576987505\n",
      "training @ iter =  186300\n",
      "0.356185227633\n",
      "training @ iter =  186400\n",
      "0.340795755386\n",
      "epoch 373, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  186500\n",
      "0.391312867403\n",
      "training @ iter =  186600\n",
      "0.378804028034\n",
      "training @ iter =  186700\n",
      "0.459141999483\n",
      "training @ iter =  186800\n",
      "0.401773422956\n",
      "training @ iter =  186900\n",
      "0.244307503104\n",
      "epoch 374, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  187000\n",
      "0.418496847153\n",
      "training @ iter =  187100\n",
      "0.292358696461\n",
      "training @ iter =  187200\n",
      "0.280748218298\n",
      "training @ iter =  187300\n",
      "0.364092051983\n",
      "training @ iter =  187400\n",
      "0.266204059124\n",
      "epoch 375, minibatch 500/500, test error 14.740000 %\n",
      "training @ iter =  187500\n",
      "0.333113282919\n",
      "training @ iter =  187600\n",
      "0.334215968847\n",
      "training @ iter =  187700\n",
      "0.310501903296\n",
      "training @ iter =  187800\n",
      "0.455541014671\n",
      "training @ iter =  187900\n",
      "0.335597604513\n",
      "epoch 376, minibatch 500/500, test error 14.740000 %\n",
      "training @ iter =  188000\n",
      "0.407913804054\n",
      "training @ iter =  188100\n",
      "0.297231584787\n",
      "training @ iter =  188200\n",
      "0.364010155201\n",
      "training @ iter =  188300\n",
      "0.458787113428\n",
      "training @ iter =  188400\n",
      "0.305187106133\n",
      "epoch 377, minibatch 500/500, test error 14.750000 %\n",
      "training @ iter =  188500\n",
      "0.348059684038\n",
      "training @ iter =  188600\n",
      "0.268473267555\n",
      "training @ iter =  188700\n",
      "0.324181377888\n",
      "training @ iter =  188800\n",
      "0.34683662653\n",
      "training @ iter =  188900\n",
      "0.277175933123\n",
      "epoch 378, minibatch 500/500, test error 14.760000 %\n",
      "training @ iter =  189000\n",
      "0.343579173088\n",
      "training @ iter =  189100\n",
      "0.281981170177\n",
      "training @ iter =  189200\n",
      "0.314475446939\n",
      "training @ iter =  189300\n",
      "0.422367334366\n",
      "training @ iter =  189400\n",
      "0.27203220129\n",
      "epoch 379, minibatch 500/500, test error 14.740000 %\n",
      "training @ iter =  189500\n",
      "0.316161721945\n",
      "training @ iter =  189600\n",
      "0.244283393025\n",
      "training @ iter =  189700\n",
      "0.331025093794\n",
      "training @ iter =  189800\n",
      "0.345576941967\n",
      "training @ iter =  189900\n",
      "0.294483661652\n",
      "epoch 380, minibatch 500/500, test error 14.720000 %\n",
      "training @ iter =  190000\n",
      "0.441445618868\n",
      "training @ iter =  190100\n",
      "0.329600721598\n",
      "training @ iter =  190200\n",
      "0.244286730886\n",
      "training @ iter =  190300\n",
      "0.361106872559\n",
      "training @ iter =  190400\n",
      "0.328313738108\n",
      "epoch 381, minibatch 500/500, test error 14.720000 %\n",
      "training @ iter =  190500\n",
      "0.46472594142\n",
      "training @ iter =  190600\n",
      "0.285819262266\n",
      "training @ iter =  190700\n",
      "0.260246127844\n",
      "training @ iter =  190800\n",
      "0.351020425558\n",
      "training @ iter =  190900\n",
      "0.282753586769\n",
      "epoch 382, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  191000\n",
      "0.395701825619\n",
      "training @ iter =  191100\n",
      "0.298601537943\n",
      "training @ iter =  191200\n",
      "0.31256455183\n",
      "training @ iter =  191300\n",
      "0.382092565298\n",
      "training @ iter =  191400\n",
      "0.290172159672\n",
      "epoch 383, minibatch 500/500, test error 14.710000 %\n",
      "training @ iter =  191500\n",
      "0.434182822704\n",
      "training @ iter =  191600\n",
      "0.302745133638\n",
      "training @ iter =  191700\n",
      "0.251615434885\n",
      "training @ iter =  191800\n",
      "0.346296310425\n",
      "training @ iter =  191900\n",
      "0.230014041066\n",
      "epoch 384, minibatch 500/500, test error 14.680000 %\n",
      "training @ iter =  192000\n",
      "0.3733279109\n",
      "training @ iter =  192100\n",
      "0.314272791147\n",
      "training @ iter =  192200\n",
      "0.384529411793\n",
      "training @ iter =  192300\n",
      "0.336576461792\n",
      "training @ iter =  192400\n",
      "0.378694027662\n",
      "epoch 385, minibatch 500/500, test error 14.640000 %\n",
      "training @ iter =  192500\n",
      "0.338948428631\n",
      "training @ iter =  192600\n",
      "0.359917342663\n",
      "training @ iter =  192700\n",
      "0.323076277971\n",
      "training @ iter =  192800\n",
      "0.341490864754\n",
      "training @ iter =  192900\n",
      "0.245443806052\n",
      "epoch 386, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  193000\n",
      "0.393540740013\n",
      "training @ iter =  193100\n",
      "0.22482919693\n",
      "training @ iter =  193200\n",
      "0.325508385897\n",
      "training @ iter =  193300\n",
      "0.35066613555\n",
      "training @ iter =  193400\n",
      "0.290232628584\n",
      "epoch 387, minibatch 500/500, test error 14.650000 %\n",
      "training @ iter =  193500\n",
      "0.38824993372\n",
      "training @ iter =  193600\n",
      "0.250664204359\n",
      "training @ iter =  193700\n",
      "0.277725815773\n",
      "training @ iter =  193800\n",
      "0.339591145515\n",
      "training @ iter =  193900\n",
      "0.309699237347\n",
      "epoch 388, minibatch 500/500, test error 14.660000 %\n",
      "training @ iter =  194000\n",
      "0.292815983295\n",
      "training @ iter =  194100\n",
      "0.340900868177\n",
      "training @ iter =  194200\n",
      "0.321683287621\n",
      "training @ iter =  194300\n",
      "0.341361165047\n",
      "training @ iter =  194400\n",
      "0.254753082991\n",
      "epoch 389, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  194500\n",
      "0.480270087719\n",
      "training @ iter =  194600\n",
      "0.307050377131\n",
      "training @ iter =  194700\n",
      "0.296257615089\n",
      "training @ iter =  194800\n",
      "0.536372482777\n",
      "training @ iter =  194900\n",
      "0.239352583885\n",
      "epoch 390, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  195000\n",
      "0.364275336266\n",
      "training @ iter =  195100\n",
      "0.416246026754\n",
      "training @ iter =  195200\n",
      "0.332495808601\n",
      "training @ iter =  195300\n",
      "0.339450746775\n",
      "training @ iter =  195400\n",
      "0.310258775949\n",
      "epoch 391, minibatch 500/500, test error 14.730000 %\n",
      "training @ iter =  195500\n",
      "0.447943717241\n",
      "training @ iter =  195600\n",
      "0.223173826933\n",
      "training @ iter =  195700\n",
      "0.389374911785\n",
      "training @ iter =  195800\n",
      "0.531020224094\n",
      "training @ iter =  195900\n",
      "0.482173323631\n",
      "epoch 392, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  196000\n",
      "0.384155511856\n",
      "training @ iter =  196100\n",
      "0.32125556469\n",
      "training @ iter =  196200\n",
      "0.32971817255\n",
      "training @ iter =  196300\n",
      "0.510311424732\n",
      "training @ iter =  196400\n",
      "0.284769892693\n",
      "epoch 393, minibatch 500/500, test error 14.700000 %\n",
      "training @ iter =  196500\n",
      "0.413541436195\n",
      "training @ iter =  196600\n",
      "0.337567597628\n",
      "training @ iter =  196700\n",
      "0.248497202992\n",
      "training @ iter =  196800\n",
      "0.442675322294\n",
      "training @ iter =  196900\n",
      "0.266932457685\n",
      "epoch 394, minibatch 500/500, test error 14.770000 %\n",
      "training @ iter =  197000\n",
      "0.299248337746\n",
      "training @ iter =  197100\n",
      "0.348319321871\n",
      "training @ iter =  197200\n",
      "0.381012260914\n",
      "training @ iter =  197300\n",
      "0.359656184912\n",
      "training @ iter =  197400\n",
      "0.252498447895\n",
      "epoch 395, minibatch 500/500, test error 14.720000 %\n",
      "training @ iter =  197500\n",
      "0.30604544282\n",
      "training @ iter =  197600\n",
      "0.380375355482\n",
      "training @ iter =  197700\n",
      "0.268135786057\n",
      "training @ iter =  197800\n",
      "0.395236670971\n",
      "training @ iter =  197900\n",
      "0.325817793608\n",
      "epoch 396, minibatch 500/500, test error 14.650000 %\n",
      "training @ iter =  198000\n",
      "0.406124264002\n",
      "training @ iter =  198100\n",
      "0.237302437425\n",
      "training @ iter =  198200\n",
      "0.265960246325\n",
      "training @ iter =  198300\n",
      "0.509093344212\n",
      "training @ iter =  198400\n",
      "0.293156176805\n",
      "epoch 397, minibatch 500/500, test error 14.690000 %\n",
      "training @ iter =  198500\n",
      "0.392659753561\n",
      "training @ iter =  198600\n",
      "0.232438743114\n",
      "training @ iter =  198700\n",
      "0.349912345409\n",
      "training @ iter =  198800\n",
      "0.305097818375\n",
      "training @ iter =  198900\n",
      "0.274242162704\n",
      "epoch 398, minibatch 500/500, test error 14.680000 %\n",
      "training @ iter =  199000\n",
      "0.394926995039\n",
      "training @ iter =  199100\n",
      "0.30592623353\n",
      "training @ iter =  199200\n",
      "0.260685145855\n",
      "training @ iter =  199300\n",
      "0.394595861435\n",
      "training @ iter =  199400\n",
      "0.385252863169\n",
      "epoch 399, minibatch 500/500, test error 14.670000 %\n",
      "training @ iter =  199500\n",
      "0.340877950191\n",
      "training @ iter =  199600\n",
      "0.221623301506\n",
      "training @ iter =  199700\n",
      "0.289197891951\n",
      "training @ iter =  199800\n",
      "0.403386622667\n",
      "training @ iter =  199900\n",
      "0.255705744028\n",
      "epoch 400, minibatch 500/500, test error 14.680000 %\n",
      "Optimization complete.\n",
      "Best test score of 14.500000 % obtained at iteration 168000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The training process for function test_ConvHighway ran for 466.66m\n"
     ]
    }
   ],
   "source": [
    "from ConvHighwayNetworks import test_ConvHighway\n",
    "from project_nn import load_data_SVHN\n",
    "\n",
    "data = load_data_SVHN(ds_rate=None,theano_shared=True, validation=False)\n",
    "test_ConvHighway(data, model=11, learning_rate=0.025, lr_decay=0.1, momentum=0.9, step_values = [100000, 150000, 175000], \n",
    "                 n_epochs=400, drop_rate=0.2, batch_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960M (CNMeM is disabled, cuDNN 5103)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 3276800 bytes of device memory (out of memory).\nApply node that caused the error: GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}(GpuDnnConv{algo='small', inplace=True}.0, GpuDimShuffle{x,0,x,x}.0, GpuElemwise{Composite{(i0 * (i1 + Abs(i1)))},no_inplace}.0)\nToposort index: 843\nInputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, (True, False, True, True)), CudaNdarrayType(float32, 4D)]\nInputs shapes: [(100, 32, 16, 16), (1, 32, 1, 1), (100, 32, 16, 16)]\nInputs strides: [(8192, 256, 16, 1), (0, 1, 0, 0), (8192, 256, 16, 1)]\nInputs values: ['not shown', 'not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{(i0 * (i1 + Abs(i1)))},no_inplace}(CudaNdarrayConstant{[[[[ 0.5]]]]}, GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}.0), GpuElemwise{Composite{((i0 * i1) + (i0 * i1 * sgn(i2)))}}[(0, 1)](CudaNdarrayConstant{[[[[ 0.5]]]]}, GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f10e1fe0cb43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_SVHN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheano_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m test_ConvHighway(data, model=13, learning_rate=0.025, lr_decay=0.1, momentum=0.9, step_values = [20000, 40000, 60000, 80000], \n\u001b[0;32m----> 6\u001b[0;31m                  n_epochs=400, drop_rate=0.2, batch_size=100, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/home/marek/Desktop/School/Columbia/NNDL-Project/src/ConvHighwayNetworks.pyc\u001b[0m in \u001b[0;36mtest_ConvHighway\u001b[0;34m(datasets, model, learning_rate, lr_decay, momentum, step_values, n_epochs, b_T, drop_rate, batch_size, verbose)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     \u001b[0mtrain_nn_NoValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_train_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/marek/Desktop/School/Columbia/NNDL-Project/src/project_nn.pyc\u001b[0m in \u001b[0;36mtrain_nn_NoValidation\u001b[0;34m(train_model, test_model, n_train_batches, n_test_batches, n_epochs, verbose, augment_data)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m             \u001b[0mcost_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training @ iter = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    872\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 3276800 bytes of device memory (out of memory).\nApply node that caused the error: GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}(GpuDnnConv{algo='small', inplace=True}.0, GpuDimShuffle{x,0,x,x}.0, GpuElemwise{Composite{(i0 * (i1 + Abs(i1)))},no_inplace}.0)\nToposort index: 843\nInputs types: [CudaNdarrayType(float32, 4D), CudaNdarrayType(float32, (True, False, True, True)), CudaNdarrayType(float32, 4D)]\nInputs shapes: [(100, 32, 16, 16), (1, 32, 1, 1), (100, 32, 16, 16)]\nInputs strides: [(8192, 256, 16, 1), (0, 1, 0, 0), (8192, 256, 16, 1)]\nInputs values: ['not shown', 'not shown', 'not shown']\nOutputs clients: [[GpuElemwise{Composite{(i0 * (i1 + Abs(i1)))},no_inplace}(CudaNdarrayConstant{[[[[ 0.5]]]]}, GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}.0), GpuElemwise{Composite{((i0 * i1) + (i0 * i1 * sgn(i2)))}}[(0, 1)](CudaNdarrayConstant{[[[[ 0.5]]]]}, GpuDnnConvGradI{algo='none', inplace=True}.0, GpuElemwise{Composite{((i0 + i1) + i2)},no_inplace}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "from ConvHighwayNetworks import test_ConvHighway\n",
    "from project_nn import load_data_SVHN\n",
    "\n",
    "data = load_data_SVHN(ds_rate=None,theano_shared=True, validation=False)\n",
    "test_ConvHighway(data, model=13, learning_rate=0.025, lr_decay=0.1, momentum=0.9, step_values = [20000, 40000, 60000, 80000], \n",
    "                 n_epochs=400, drop_rate=0.2, batch_size=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
